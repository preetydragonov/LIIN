{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [****  목차   ****]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < 문제 명세 >\n",
    "\n",
    "- 문제 명세\n",
    "\n",
    "\n",
    "## < Settings >\n",
    "\n",
    "- import 및 경로 지정\n",
    "- 피클된 데이터 꺼내오기\n",
    "- dirty .txt file preprocessed to pretty .csv file\n",
    "- crosstab\n",
    "- data split[training, validation, test]\n",
    "\n",
    "\n",
    "## < analysis >\n",
    "\n",
    "\n",
    "#### [1] input\n",
    "\n",
    "- question by question correlation\n",
    ": pearson, kendall Tau, 단순유사도\n",
    "- User by User correlation\n",
    ": pearson, kendall Tau, 단순유사도\n",
    "\n",
    "\n",
    "#### [2] Score Function\n",
    "\n",
    "- collaborative filtering with neural network\n",
    "\n",
    "\n",
    "#### [3] Loss Function\n",
    "\n",
    "- cross entropy with softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*********"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문제 명세"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 심리테스트 고객 오ㅇㅇ님이 question 37에 문제에 Yes라고 대답할지 No라고 대답할지 예측해보기?\n",
    "\n",
    "- 어떤 상황이든지 예측을 하려면, 오 ㅇㅇ 고객으로부터 데이터를 받아야 합니다. 마치 영화추천 사이트가 새로운 가입한 고객으로부터 그 고객이 좋아하는 영화에 대한 도메인 지식을 받는 것처럼요.\n",
    "\n",
    "\n",
    "\n",
    "- 하지만, 영화추천 사이트에서 만들어야하는 밸류와 제가 만들어내야 하는 밸류는 다릅니다. 주어진 데이터가 다르기 때문입니다. 영화추천 사이트는 이빠진 데이터에를 채우는 것이 밸류의 원천입니다. 하지만, 제게 주어진 데이터는 이빠진 데이터가 아닙니다. 49192명이 100문제씩 완벽하게 풀어낸 Full 데이터셋입니다. 제가 가지고 있는 데이터셋으로부터는 다른 밸류를 창출해야 했습니다.\n",
    "\n",
    "\n",
    "\n",
    "- 그래서 제가 생각했던 것은 고객으로부터 데이터 입력을 최소화하는 것이었습니다. 고객으로부터 받아내는 데이터의 양을 최소화해보는 것이 해결해보고 싶은 문제였습니다. 필요한 최소한의 질문만 하고서, 고객으로부터 최대한의 정보를 끌어내는 것이 목표였습니다. 다시 말해서, 100문제를 다 물어보지 않고, 고객의 성향을 유추할 수 있는 핵심 문제(10~30여개)만 물어보고서, 고객이 다른 질문에 대해서 어떻게 답변할지 예측해내는 것이 목표였습니다.\n",
    "\n",
    "\n",
    "\n",
    "- 어떻게 그 핵심 문제 10 ~ 30 개 를 골라낼 것인가. 이것이 해결하고 싶은 문제였습니다. 해당 10 ~ 30 문제는 나머지 70 ~ 90 문제를 예측할 수 있는 질문들이어야 합니다.\n",
    "\n",
    "\n",
    "\n",
    "- 문제 별로 양의 상관 관계, 음의 상관 관계 두 가지를 바탕으로 예측할 수 있을 것입니다. 양의 상관관계는 예를 들어, \"Q3. 영양제를 챙겨 드십니까?\"에 yes라고 대답한 사람은 \"Q54. 건강을 챙기는 것이 중요합니까?\"라는 질문에도 yes라고 대답할 확률이 높은 것처럼 말입니다. 음의 상관관계는 반대의 경우가 되겠지요. 예를 들어, 앞선 Q3에 yes라고 한 사람은 \"Q75. 건강을 챙기는 것이 불필요합니까?\"라는 질문에 No라고 할 것입니다.\n",
    "\n",
    "\n",
    "\n",
    "- 또한, 유저들을 클러스터링 해서 예측할 수 있을 것입니다. 예를 들어 \"Q3. 아침 식사를 하십니까?\"에 yes라고 대답한 A가 ㄲㄲㄲ유형에 속한다면, ㄲㄲㄲ유형에 속하는 또 다른 유저 B도 해당 질문에 yes라고 대답할 확률이 높아질 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1줄 요약: 100문제가 아닌 10~20여 문제만 물어봐도 나머지 문제에 대한 추측이 가능하도록!!!!! 핵심 문제를 추려내는 것이 이 데이터셋으로부터 얻을 수 있는 가치다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1/5] import 및 경로 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "from theano.sandbox import cuda\n",
    "\n",
    "import theano\n",
    "from theano import shared, tensor as T\n",
    "from theano.tensor.nnet import conv2d, nnet\n",
    "from theano.tensor.signal import pool\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.utils import np_utils\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Reshape, merge, LSTM, Bidirectional\n",
    "from keras.layers import TimeDistributed, Activation, SimpleRNN, GRU\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers import Merge\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "from keras.metrics import categorical_crossentropy, categorical_accuracy\n",
    "from keras.layers.convolutional import *\n",
    "from keras.preprocessing import image, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import regularizers\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as cp\n",
    "import sklearn as sk\n",
    "from scipy import stats\n",
    "from scipy.sparse import csr_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "path = \"/Users/choelinbeom/courses/deeplearning1/data/mind_user_answers/workfile.csv\"\n",
    "model_path = \"/Users/choelinbeom/courses/deeplearning1/data/mind_user_answers/model/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##ratings = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [2/5] pickle된 데이터 꺼내오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = pd.read_pickle(model_path + 'ratings.pkl')\n",
    "\n",
    "array_pd = pd.read_pickle(model_path + 'array_pd.pkl')\n",
    "cross = pd.read_pickle(model_path + 'cross.pkl')\n",
    "\n",
    "QbyQ_Yes = pd.read_pickle(model_path + 'Questions_by_Questions.pkl')\n",
    "QbyQ_No = pd.read_pickle(model_path + 'Questions_by_Questions_not.pkl')\n",
    "QbyQ = QbyQ_Yes + QbyQ_No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QbyQ = QbyQ_Yes + QbyQ_No"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Questions</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Questions</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45192</td>\n",
       "      <td>21497</td>\n",
       "      <td>20168</td>\n",
       "      <td>24144</td>\n",
       "      <td>23720</td>\n",
       "      <td>21580</td>\n",
       "      <td>25513</td>\n",
       "      <td>24404</td>\n",
       "      <td>18864</td>\n",
       "      <td>23527</td>\n",
       "      <td>...</td>\n",
       "      <td>20497</td>\n",
       "      <td>22608</td>\n",
       "      <td>24048</td>\n",
       "      <td>25222</td>\n",
       "      <td>24830</td>\n",
       "      <td>23622</td>\n",
       "      <td>26351</td>\n",
       "      <td>22984</td>\n",
       "      <td>23300</td>\n",
       "      <td>21535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21497</td>\n",
       "      <td>45192</td>\n",
       "      <td>27329</td>\n",
       "      <td>17387</td>\n",
       "      <td>21923</td>\n",
       "      <td>26467</td>\n",
       "      <td>17718</td>\n",
       "      <td>20911</td>\n",
       "      <td>23281</td>\n",
       "      <td>22288</td>\n",
       "      <td>...</td>\n",
       "      <td>22452</td>\n",
       "      <td>22721</td>\n",
       "      <td>20369</td>\n",
       "      <td>20497</td>\n",
       "      <td>18505</td>\n",
       "      <td>22437</td>\n",
       "      <td>19486</td>\n",
       "      <td>26695</td>\n",
       "      <td>17007</td>\n",
       "      <td>26352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20168</td>\n",
       "      <td>27329</td>\n",
       "      <td>45192</td>\n",
       "      <td>16278</td>\n",
       "      <td>20942</td>\n",
       "      <td>29612</td>\n",
       "      <td>13241</td>\n",
       "      <td>21006</td>\n",
       "      <td>23024</td>\n",
       "      <td>17173</td>\n",
       "      <td>...</td>\n",
       "      <td>21111</td>\n",
       "      <td>26936</td>\n",
       "      <td>18126</td>\n",
       "      <td>17788</td>\n",
       "      <td>14822</td>\n",
       "      <td>24678</td>\n",
       "      <td>15329</td>\n",
       "      <td>29228</td>\n",
       "      <td>22486</td>\n",
       "      <td>30087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24144</td>\n",
       "      <td>17387</td>\n",
       "      <td>16278</td>\n",
       "      <td>45192</td>\n",
       "      <td>23090</td>\n",
       "      <td>16310</td>\n",
       "      <td>30893</td>\n",
       "      <td>24808</td>\n",
       "      <td>23562</td>\n",
       "      <td>27727</td>\n",
       "      <td>...</td>\n",
       "      <td>24787</td>\n",
       "      <td>19150</td>\n",
       "      <td>25510</td>\n",
       "      <td>28990</td>\n",
       "      <td>31174</td>\n",
       "      <td>21728</td>\n",
       "      <td>29781</td>\n",
       "      <td>15922</td>\n",
       "      <td>24080</td>\n",
       "      <td>16221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23720</td>\n",
       "      <td>21923</td>\n",
       "      <td>20942</td>\n",
       "      <td>23090</td>\n",
       "      <td>45192</td>\n",
       "      <td>22366</td>\n",
       "      <td>24555</td>\n",
       "      <td>24108</td>\n",
       "      <td>22790</td>\n",
       "      <td>24207</td>\n",
       "      <td>...</td>\n",
       "      <td>23605</td>\n",
       "      <td>22028</td>\n",
       "      <td>25120</td>\n",
       "      <td>23018</td>\n",
       "      <td>24006</td>\n",
       "      <td>23108</td>\n",
       "      <td>23997</td>\n",
       "      <td>22084</td>\n",
       "      <td>22280</td>\n",
       "      <td>21181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>21580</td>\n",
       "      <td>26467</td>\n",
       "      <td>29612</td>\n",
       "      <td>16310</td>\n",
       "      <td>22366</td>\n",
       "      <td>45192</td>\n",
       "      <td>14825</td>\n",
       "      <td>22294</td>\n",
       "      <td>20664</td>\n",
       "      <td>13603</td>\n",
       "      <td>...</td>\n",
       "      <td>18883</td>\n",
       "      <td>30978</td>\n",
       "      <td>18752</td>\n",
       "      <td>17876</td>\n",
       "      <td>15364</td>\n",
       "      <td>28548</td>\n",
       "      <td>14221</td>\n",
       "      <td>34142</td>\n",
       "      <td>24886</td>\n",
       "      <td>32497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25513</td>\n",
       "      <td>17718</td>\n",
       "      <td>13241</td>\n",
       "      <td>30893</td>\n",
       "      <td>24555</td>\n",
       "      <td>14825</td>\n",
       "      <td>45192</td>\n",
       "      <td>24877</td>\n",
       "      <td>23723</td>\n",
       "      <td>30876</td>\n",
       "      <td>...</td>\n",
       "      <td>25562</td>\n",
       "      <td>16737</td>\n",
       "      <td>28301</td>\n",
       "      <td>29099</td>\n",
       "      <td>32965</td>\n",
       "      <td>20305</td>\n",
       "      <td>31600</td>\n",
       "      <td>14873</td>\n",
       "      <td>22295</td>\n",
       "      <td>13502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24404</td>\n",
       "      <td>20911</td>\n",
       "      <td>21006</td>\n",
       "      <td>24808</td>\n",
       "      <td>24108</td>\n",
       "      <td>22294</td>\n",
       "      <td>24877</td>\n",
       "      <td>45192</td>\n",
       "      <td>22086</td>\n",
       "      <td>22599</td>\n",
       "      <td>...</td>\n",
       "      <td>22343</td>\n",
       "      <td>25084</td>\n",
       "      <td>23952</td>\n",
       "      <td>25202</td>\n",
       "      <td>25450</td>\n",
       "      <td>24624</td>\n",
       "      <td>23927</td>\n",
       "      <td>22396</td>\n",
       "      <td>24808</td>\n",
       "      <td>22297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18864</td>\n",
       "      <td>23281</td>\n",
       "      <td>23024</td>\n",
       "      <td>23562</td>\n",
       "      <td>22790</td>\n",
       "      <td>20664</td>\n",
       "      <td>23723</td>\n",
       "      <td>22086</td>\n",
       "      <td>45192</td>\n",
       "      <td>27179</td>\n",
       "      <td>...</td>\n",
       "      <td>27065</td>\n",
       "      <td>19410</td>\n",
       "      <td>24076</td>\n",
       "      <td>23112</td>\n",
       "      <td>24430</td>\n",
       "      <td>20528</td>\n",
       "      <td>23313</td>\n",
       "      <td>19750</td>\n",
       "      <td>20558</td>\n",
       "      <td>20683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>23527</td>\n",
       "      <td>22288</td>\n",
       "      <td>17173</td>\n",
       "      <td>27727</td>\n",
       "      <td>24207</td>\n",
       "      <td>13603</td>\n",
       "      <td>30876</td>\n",
       "      <td>22599</td>\n",
       "      <td>27179</td>\n",
       "      <td>45192</td>\n",
       "      <td>...</td>\n",
       "      <td>29898</td>\n",
       "      <td>8767</td>\n",
       "      <td>30943</td>\n",
       "      <td>25981</td>\n",
       "      <td>30335</td>\n",
       "      <td>14265</td>\n",
       "      <td>32408</td>\n",
       "      <td>12831</td>\n",
       "      <td>16427</td>\n",
       "      <td>10152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>20747</td>\n",
       "      <td>23890</td>\n",
       "      <td>24253</td>\n",
       "      <td>21687</td>\n",
       "      <td>23237</td>\n",
       "      <td>26011</td>\n",
       "      <td>21482</td>\n",
       "      <td>23269</td>\n",
       "      <td>25041</td>\n",
       "      <td>22642</td>\n",
       "      <td>...</td>\n",
       "      <td>24658</td>\n",
       "      <td>23299</td>\n",
       "      <td>24303</td>\n",
       "      <td>22259</td>\n",
       "      <td>22789</td>\n",
       "      <td>23493</td>\n",
       "      <td>20778</td>\n",
       "      <td>23901</td>\n",
       "      <td>22733</td>\n",
       "      <td>24132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>23521</td>\n",
       "      <td>23824</td>\n",
       "      <td>19157</td>\n",
       "      <td>27289</td>\n",
       "      <td>22641</td>\n",
       "      <td>16573</td>\n",
       "      <td>28714</td>\n",
       "      <td>23063</td>\n",
       "      <td>25667</td>\n",
       "      <td>33988</td>\n",
       "      <td>...</td>\n",
       "      <td>27472</td>\n",
       "      <td>13025</td>\n",
       "      <td>27767</td>\n",
       "      <td>26869</td>\n",
       "      <td>28849</td>\n",
       "      <td>17607</td>\n",
       "      <td>29974</td>\n",
       "      <td>16069</td>\n",
       "      <td>18475</td>\n",
       "      <td>14404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>23790</td>\n",
       "      <td>26739</td>\n",
       "      <td>27984</td>\n",
       "      <td>17360</td>\n",
       "      <td>22914</td>\n",
       "      <td>32738</td>\n",
       "      <td>16849</td>\n",
       "      <td>23106</td>\n",
       "      <td>21360</td>\n",
       "      <td>18597</td>\n",
       "      <td>...</td>\n",
       "      <td>20807</td>\n",
       "      <td>26620</td>\n",
       "      <td>21108</td>\n",
       "      <td>19444</td>\n",
       "      <td>17340</td>\n",
       "      <td>24986</td>\n",
       "      <td>17531</td>\n",
       "      <td>31844</td>\n",
       "      <td>22232</td>\n",
       "      <td>29011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25595</td>\n",
       "      <td>22604</td>\n",
       "      <td>20435</td>\n",
       "      <td>25981</td>\n",
       "      <td>24857</td>\n",
       "      <td>21651</td>\n",
       "      <td>26790</td>\n",
       "      <td>25439</td>\n",
       "      <td>23193</td>\n",
       "      <td>27192</td>\n",
       "      <td>...</td>\n",
       "      <td>24070</td>\n",
       "      <td>20261</td>\n",
       "      <td>26097</td>\n",
       "      <td>25609</td>\n",
       "      <td>27135</td>\n",
       "      <td>23891</td>\n",
       "      <td>26594</td>\n",
       "      <td>21251</td>\n",
       "      <td>21833</td>\n",
       "      <td>19674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26375</td>\n",
       "      <td>18624</td>\n",
       "      <td>16929</td>\n",
       "      <td>28623</td>\n",
       "      <td>23747</td>\n",
       "      <td>18167</td>\n",
       "      <td>29132</td>\n",
       "      <td>25037</td>\n",
       "      <td>21343</td>\n",
       "      <td>24806</td>\n",
       "      <td>...</td>\n",
       "      <td>20590</td>\n",
       "      <td>21149</td>\n",
       "      <td>25395</td>\n",
       "      <td>27267</td>\n",
       "      <td>28299</td>\n",
       "      <td>22621</td>\n",
       "      <td>32446</td>\n",
       "      <td>18917</td>\n",
       "      <td>24727</td>\n",
       "      <td>18070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>24239</td>\n",
       "      <td>17924</td>\n",
       "      <td>19357</td>\n",
       "      <td>25139</td>\n",
       "      <td>25305</td>\n",
       "      <td>21961</td>\n",
       "      <td>26948</td>\n",
       "      <td>24801</td>\n",
       "      <td>21997</td>\n",
       "      <td>23030</td>\n",
       "      <td>...</td>\n",
       "      <td>22968</td>\n",
       "      <td>23311</td>\n",
       "      <td>27803</td>\n",
       "      <td>23743</td>\n",
       "      <td>26103</td>\n",
       "      <td>24677</td>\n",
       "      <td>24656</td>\n",
       "      <td>21353</td>\n",
       "      <td>25787</td>\n",
       "      <td>20548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>24429</td>\n",
       "      <td>19314</td>\n",
       "      <td>16095</td>\n",
       "      <td>31309</td>\n",
       "      <td>22597</td>\n",
       "      <td>15359</td>\n",
       "      <td>31582</td>\n",
       "      <td>25097</td>\n",
       "      <td>24559</td>\n",
       "      <td>30108</td>\n",
       "      <td>...</td>\n",
       "      <td>25504</td>\n",
       "      <td>17323</td>\n",
       "      <td>26303</td>\n",
       "      <td>31415</td>\n",
       "      <td>31995</td>\n",
       "      <td>19565</td>\n",
       "      <td>30474</td>\n",
       "      <td>15265</td>\n",
       "      <td>22525</td>\n",
       "      <td>15178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>24692</td>\n",
       "      <td>22193</td>\n",
       "      <td>18154</td>\n",
       "      <td>27692</td>\n",
       "      <td>23828</td>\n",
       "      <td>17886</td>\n",
       "      <td>29271</td>\n",
       "      <td>24672</td>\n",
       "      <td>23910</td>\n",
       "      <td>29603</td>\n",
       "      <td>...</td>\n",
       "      <td>24509</td>\n",
       "      <td>17528</td>\n",
       "      <td>26220</td>\n",
       "      <td>27690</td>\n",
       "      <td>28810</td>\n",
       "      <td>20410</td>\n",
       "      <td>29405</td>\n",
       "      <td>18030</td>\n",
       "      <td>20892</td>\n",
       "      <td>16923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24704</td>\n",
       "      <td>21227</td>\n",
       "      <td>19002</td>\n",
       "      <td>26358</td>\n",
       "      <td>23732</td>\n",
       "      <td>21452</td>\n",
       "      <td>26875</td>\n",
       "      <td>25472</td>\n",
       "      <td>22278</td>\n",
       "      <td>22733</td>\n",
       "      <td>...</td>\n",
       "      <td>22495</td>\n",
       "      <td>24188</td>\n",
       "      <td>23666</td>\n",
       "      <td>26798</td>\n",
       "      <td>27406</td>\n",
       "      <td>26608</td>\n",
       "      <td>24373</td>\n",
       "      <td>21548</td>\n",
       "      <td>24164</td>\n",
       "      <td>21939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>26190</td>\n",
       "      <td>22125</td>\n",
       "      <td>19780</td>\n",
       "      <td>25934</td>\n",
       "      <td>22770</td>\n",
       "      <td>19538</td>\n",
       "      <td>27483</td>\n",
       "      <td>23398</td>\n",
       "      <td>23282</td>\n",
       "      <td>29661</td>\n",
       "      <td>...</td>\n",
       "      <td>24793</td>\n",
       "      <td>16564</td>\n",
       "      <td>26440</td>\n",
       "      <td>25666</td>\n",
       "      <td>26326</td>\n",
       "      <td>19450</td>\n",
       "      <td>29055</td>\n",
       "      <td>19480</td>\n",
       "      <td>20710</td>\n",
       "      <td>17621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>24501</td>\n",
       "      <td>21580</td>\n",
       "      <td>16461</td>\n",
       "      <td>28435</td>\n",
       "      <td>24281</td>\n",
       "      <td>13781</td>\n",
       "      <td>31700</td>\n",
       "      <td>23441</td>\n",
       "      <td>25951</td>\n",
       "      <td>37918</td>\n",
       "      <td>...</td>\n",
       "      <td>29164</td>\n",
       "      <td>9793</td>\n",
       "      <td>30959</td>\n",
       "      <td>27647</td>\n",
       "      <td>31047</td>\n",
       "      <td>14575</td>\n",
       "      <td>33246</td>\n",
       "      <td>13095</td>\n",
       "      <td>17323</td>\n",
       "      <td>10342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21825</td>\n",
       "      <td>27342</td>\n",
       "      <td>27693</td>\n",
       "      <td>17167</td>\n",
       "      <td>22981</td>\n",
       "      <td>28263</td>\n",
       "      <td>16932</td>\n",
       "      <td>21945</td>\n",
       "      <td>21397</td>\n",
       "      <td>17932</td>\n",
       "      <td>...</td>\n",
       "      <td>21046</td>\n",
       "      <td>26679</td>\n",
       "      <td>20553</td>\n",
       "      <td>19257</td>\n",
       "      <td>16723</td>\n",
       "      <td>24787</td>\n",
       "      <td>15902</td>\n",
       "      <td>29047</td>\n",
       "      <td>21215</td>\n",
       "      <td>28966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23809</td>\n",
       "      <td>22276</td>\n",
       "      <td>21441</td>\n",
       "      <td>24015</td>\n",
       "      <td>23923</td>\n",
       "      <td>22413</td>\n",
       "      <td>24552</td>\n",
       "      <td>24355</td>\n",
       "      <td>23553</td>\n",
       "      <td>22720</td>\n",
       "      <td>...</td>\n",
       "      <td>22816</td>\n",
       "      <td>24395</td>\n",
       "      <td>22295</td>\n",
       "      <td>25193</td>\n",
       "      <td>24999</td>\n",
       "      <td>23393</td>\n",
       "      <td>23284</td>\n",
       "      <td>22701</td>\n",
       "      <td>23231</td>\n",
       "      <td>23366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>22786</td>\n",
       "      <td>24437</td>\n",
       "      <td>21682</td>\n",
       "      <td>25126</td>\n",
       "      <td>21748</td>\n",
       "      <td>18864</td>\n",
       "      <td>24645</td>\n",
       "      <td>23308</td>\n",
       "      <td>24472</td>\n",
       "      <td>27779</td>\n",
       "      <td>...</td>\n",
       "      <td>24837</td>\n",
       "      <td>18586</td>\n",
       "      <td>20196</td>\n",
       "      <td>26716</td>\n",
       "      <td>25490</td>\n",
       "      <td>19068</td>\n",
       "      <td>25747</td>\n",
       "      <td>19480</td>\n",
       "      <td>19550</td>\n",
       "      <td>19417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24121</td>\n",
       "      <td>18914</td>\n",
       "      <td>18313</td>\n",
       "      <td>27651</td>\n",
       "      <td>23639</td>\n",
       "      <td>18679</td>\n",
       "      <td>27738</td>\n",
       "      <td>26839</td>\n",
       "      <td>22675</td>\n",
       "      <td>24174</td>\n",
       "      <td>...</td>\n",
       "      <td>23094</td>\n",
       "      <td>22443</td>\n",
       "      <td>24231</td>\n",
       "      <td>28081</td>\n",
       "      <td>27923</td>\n",
       "      <td>22761</td>\n",
       "      <td>26554</td>\n",
       "      <td>19265</td>\n",
       "      <td>25265</td>\n",
       "      <td>19578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>24201</td>\n",
       "      <td>23188</td>\n",
       "      <td>21979</td>\n",
       "      <td>23913</td>\n",
       "      <td>21277</td>\n",
       "      <td>20415</td>\n",
       "      <td>24128</td>\n",
       "      <td>22617</td>\n",
       "      <td>22751</td>\n",
       "      <td>25820</td>\n",
       "      <td>...</td>\n",
       "      <td>23946</td>\n",
       "      <td>19833</td>\n",
       "      <td>23379</td>\n",
       "      <td>24497</td>\n",
       "      <td>24141</td>\n",
       "      <td>21217</td>\n",
       "      <td>24656</td>\n",
       "      <td>21035</td>\n",
       "      <td>21385</td>\n",
       "      <td>20390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>24265</td>\n",
       "      <td>16668</td>\n",
       "      <td>15907</td>\n",
       "      <td>29643</td>\n",
       "      <td>24563</td>\n",
       "      <td>14601</td>\n",
       "      <td>31266</td>\n",
       "      <td>24279</td>\n",
       "      <td>24349</td>\n",
       "      <td>32560</td>\n",
       "      <td>...</td>\n",
       "      <td>26898</td>\n",
       "      <td>14371</td>\n",
       "      <td>29727</td>\n",
       "      <td>26815</td>\n",
       "      <td>30127</td>\n",
       "      <td>18021</td>\n",
       "      <td>31274</td>\n",
       "      <td>14103</td>\n",
       "      <td>21567</td>\n",
       "      <td>12178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28756</td>\n",
       "      <td>21939</td>\n",
       "      <td>20042</td>\n",
       "      <td>23464</td>\n",
       "      <td>23974</td>\n",
       "      <td>23754</td>\n",
       "      <td>25135</td>\n",
       "      <td>24600</td>\n",
       "      <td>20974</td>\n",
       "      <td>22305</td>\n",
       "      <td>...</td>\n",
       "      <td>21633</td>\n",
       "      <td>24152</td>\n",
       "      <td>23674</td>\n",
       "      <td>24848</td>\n",
       "      <td>24636</td>\n",
       "      <td>24458</td>\n",
       "      <td>23783</td>\n",
       "      <td>24476</td>\n",
       "      <td>23698</td>\n",
       "      <td>22991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>22272</td>\n",
       "      <td>22729</td>\n",
       "      <td>20632</td>\n",
       "      <td>26104</td>\n",
       "      <td>22770</td>\n",
       "      <td>18178</td>\n",
       "      <td>26695</td>\n",
       "      <td>22460</td>\n",
       "      <td>25558</td>\n",
       "      <td>32041</td>\n",
       "      <td>...</td>\n",
       "      <td>28013</td>\n",
       "      <td>14746</td>\n",
       "      <td>27400</td>\n",
       "      <td>24742</td>\n",
       "      <td>27058</td>\n",
       "      <td>18332</td>\n",
       "      <td>27553</td>\n",
       "      <td>16826</td>\n",
       "      <td>19228</td>\n",
       "      <td>15789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>25736</td>\n",
       "      <td>17603</td>\n",
       "      <td>13376</td>\n",
       "      <td>30534</td>\n",
       "      <td>24620</td>\n",
       "      <td>13872</td>\n",
       "      <td>32903</td>\n",
       "      <td>25042</td>\n",
       "      <td>23272</td>\n",
       "      <td>29567</td>\n",
       "      <td>...</td>\n",
       "      <td>24811</td>\n",
       "      <td>17198</td>\n",
       "      <td>27848</td>\n",
       "      <td>28914</td>\n",
       "      <td>31746</td>\n",
       "      <td>20144</td>\n",
       "      <td>32019</td>\n",
       "      <td>14360</td>\n",
       "      <td>22958</td>\n",
       "      <td>13877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>21939</td>\n",
       "      <td>23750</td>\n",
       "      <td>22085</td>\n",
       "      <td>21839</td>\n",
       "      <td>25839</td>\n",
       "      <td>22775</td>\n",
       "      <td>22954</td>\n",
       "      <td>22835</td>\n",
       "      <td>23951</td>\n",
       "      <td>26472</td>\n",
       "      <td>...</td>\n",
       "      <td>24820</td>\n",
       "      <td>19347</td>\n",
       "      <td>26067</td>\n",
       "      <td>21033</td>\n",
       "      <td>22839</td>\n",
       "      <td>20387</td>\n",
       "      <td>23786</td>\n",
       "      <td>21607</td>\n",
       "      <td>19993</td>\n",
       "      <td>20278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>26748</td>\n",
       "      <td>19535</td>\n",
       "      <td>17492</td>\n",
       "      <td>28112</td>\n",
       "      <td>23722</td>\n",
       "      <td>18986</td>\n",
       "      <td>28679</td>\n",
       "      <td>24606</td>\n",
       "      <td>21160</td>\n",
       "      <td>23313</td>\n",
       "      <td>...</td>\n",
       "      <td>20359</td>\n",
       "      <td>22778</td>\n",
       "      <td>23686</td>\n",
       "      <td>28008</td>\n",
       "      <td>28100</td>\n",
       "      <td>24014</td>\n",
       "      <td>29419</td>\n",
       "      <td>20226</td>\n",
       "      <td>24736</td>\n",
       "      <td>19819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>24234</td>\n",
       "      <td>21693</td>\n",
       "      <td>20568</td>\n",
       "      <td>26304</td>\n",
       "      <td>23424</td>\n",
       "      <td>21818</td>\n",
       "      <td>25967</td>\n",
       "      <td>24672</td>\n",
       "      <td>23418</td>\n",
       "      <td>22675</td>\n",
       "      <td>...</td>\n",
       "      <td>22423</td>\n",
       "      <td>24292</td>\n",
       "      <td>22650</td>\n",
       "      <td>27650</td>\n",
       "      <td>26252</td>\n",
       "      <td>24394</td>\n",
       "      <td>24481</td>\n",
       "      <td>21824</td>\n",
       "      <td>23772</td>\n",
       "      <td>22471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>22177</td>\n",
       "      <td>30570</td>\n",
       "      <td>25969</td>\n",
       "      <td>20669</td>\n",
       "      <td>21169</td>\n",
       "      <td>26467</td>\n",
       "      <td>18856</td>\n",
       "      <td>22889</td>\n",
       "      <td>21417</td>\n",
       "      <td>18138</td>\n",
       "      <td>...</td>\n",
       "      <td>20200</td>\n",
       "      <td>27201</td>\n",
       "      <td>17527</td>\n",
       "      <td>22865</td>\n",
       "      <td>19657</td>\n",
       "      <td>25047</td>\n",
       "      <td>19118</td>\n",
       "      <td>27461</td>\n",
       "      <td>21765</td>\n",
       "      <td>28080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>27088</td>\n",
       "      <td>23133</td>\n",
       "      <td>21212</td>\n",
       "      <td>24566</td>\n",
       "      <td>23134</td>\n",
       "      <td>22032</td>\n",
       "      <td>24321</td>\n",
       "      <td>23516</td>\n",
       "      <td>18334</td>\n",
       "      <td>22221</td>\n",
       "      <td>...</td>\n",
       "      <td>20271</td>\n",
       "      <td>22638</td>\n",
       "      <td>23084</td>\n",
       "      <td>24742</td>\n",
       "      <td>23028</td>\n",
       "      <td>23134</td>\n",
       "      <td>25169</td>\n",
       "      <td>24098</td>\n",
       "      <td>22394</td>\n",
       "      <td>22399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>23849</td>\n",
       "      <td>19442</td>\n",
       "      <td>18333</td>\n",
       "      <td>28445</td>\n",
       "      <td>22385</td>\n",
       "      <td>21033</td>\n",
       "      <td>27618</td>\n",
       "      <td>25197</td>\n",
       "      <td>21781</td>\n",
       "      <td>21170</td>\n",
       "      <td>...</td>\n",
       "      <td>21730</td>\n",
       "      <td>25671</td>\n",
       "      <td>21445</td>\n",
       "      <td>28177</td>\n",
       "      <td>27823</td>\n",
       "      <td>25847</td>\n",
       "      <td>25250</td>\n",
       "      <td>20927</td>\n",
       "      <td>26471</td>\n",
       "      <td>21682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>24609</td>\n",
       "      <td>19656</td>\n",
       "      <td>16519</td>\n",
       "      <td>30471</td>\n",
       "      <td>23553</td>\n",
       "      <td>15513</td>\n",
       "      <td>31322</td>\n",
       "      <td>24981</td>\n",
       "      <td>24991</td>\n",
       "      <td>30736</td>\n",
       "      <td>...</td>\n",
       "      <td>26098</td>\n",
       "      <td>16965</td>\n",
       "      <td>27159</td>\n",
       "      <td>29885</td>\n",
       "      <td>31341</td>\n",
       "      <td>19485</td>\n",
       "      <td>30004</td>\n",
       "      <td>15283</td>\n",
       "      <td>21939</td>\n",
       "      <td>15088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>24754</td>\n",
       "      <td>22925</td>\n",
       "      <td>21368</td>\n",
       "      <td>23944</td>\n",
       "      <td>23234</td>\n",
       "      <td>21924</td>\n",
       "      <td>24429</td>\n",
       "      <td>23304</td>\n",
       "      <td>21914</td>\n",
       "      <td>24913</td>\n",
       "      <td>...</td>\n",
       "      <td>16845</td>\n",
       "      <td>20574</td>\n",
       "      <td>24238</td>\n",
       "      <td>24350</td>\n",
       "      <td>23858</td>\n",
       "      <td>21672</td>\n",
       "      <td>28015</td>\n",
       "      <td>22436</td>\n",
       "      <td>22012</td>\n",
       "      <td>21121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>25668</td>\n",
       "      <td>19749</td>\n",
       "      <td>15374</td>\n",
       "      <td>28758</td>\n",
       "      <td>25418</td>\n",
       "      <td>16680</td>\n",
       "      <td>31907</td>\n",
       "      <td>26234</td>\n",
       "      <td>23524</td>\n",
       "      <td>29875</td>\n",
       "      <td>...</td>\n",
       "      <td>25223</td>\n",
       "      <td>17998</td>\n",
       "      <td>28320</td>\n",
       "      <td>27878</td>\n",
       "      <td>31376</td>\n",
       "      <td>21458</td>\n",
       "      <td>30145</td>\n",
       "      <td>17002</td>\n",
       "      <td>21880</td>\n",
       "      <td>15321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>23726</td>\n",
       "      <td>16299</td>\n",
       "      <td>17802</td>\n",
       "      <td>26226</td>\n",
       "      <td>24604</td>\n",
       "      <td>19450</td>\n",
       "      <td>28285</td>\n",
       "      <td>24870</td>\n",
       "      <td>22674</td>\n",
       "      <td>25889</td>\n",
       "      <td>...</td>\n",
       "      <td>24427</td>\n",
       "      <td>20430</td>\n",
       "      <td>29640</td>\n",
       "      <td>23908</td>\n",
       "      <td>27132</td>\n",
       "      <td>22302</td>\n",
       "      <td>26791</td>\n",
       "      <td>18914</td>\n",
       "      <td>25350</td>\n",
       "      <td>17449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>24250</td>\n",
       "      <td>18697</td>\n",
       "      <td>16342</td>\n",
       "      <td>31074</td>\n",
       "      <td>23874</td>\n",
       "      <td>16240</td>\n",
       "      <td>30941</td>\n",
       "      <td>25238</td>\n",
       "      <td>24910</td>\n",
       "      <td>28947</td>\n",
       "      <td>...</td>\n",
       "      <td>26141</td>\n",
       "      <td>18570</td>\n",
       "      <td>26524</td>\n",
       "      <td>28804</td>\n",
       "      <td>34076</td>\n",
       "      <td>21824</td>\n",
       "      <td>29027</td>\n",
       "      <td>15342</td>\n",
       "      <td>23460</td>\n",
       "      <td>16239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>20690</td>\n",
       "      <td>22513</td>\n",
       "      <td>22360</td>\n",
       "      <td>23132</td>\n",
       "      <td>22894</td>\n",
       "      <td>21426</td>\n",
       "      <td>23249</td>\n",
       "      <td>22698</td>\n",
       "      <td>23884</td>\n",
       "      <td>23139</td>\n",
       "      <td>...</td>\n",
       "      <td>23929</td>\n",
       "      <td>22198</td>\n",
       "      <td>22616</td>\n",
       "      <td>23126</td>\n",
       "      <td>23270</td>\n",
       "      <td>21730</td>\n",
       "      <td>22135</td>\n",
       "      <td>21192</td>\n",
       "      <td>22506</td>\n",
       "      <td>21475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>25270</td>\n",
       "      <td>24587</td>\n",
       "      <td>22082</td>\n",
       "      <td>22988</td>\n",
       "      <td>23780</td>\n",
       "      <td>24326</td>\n",
       "      <td>24127</td>\n",
       "      <td>23378</td>\n",
       "      <td>22280</td>\n",
       "      <td>25389</td>\n",
       "      <td>...</td>\n",
       "      <td>22737</td>\n",
       "      <td>21032</td>\n",
       "      <td>24834</td>\n",
       "      <td>22856</td>\n",
       "      <td>23212</td>\n",
       "      <td>23364</td>\n",
       "      <td>25133</td>\n",
       "      <td>25576</td>\n",
       "      <td>20954</td>\n",
       "      <td>22091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>25346</td>\n",
       "      <td>21177</td>\n",
       "      <td>19038</td>\n",
       "      <td>27160</td>\n",
       "      <td>23664</td>\n",
       "      <td>20352</td>\n",
       "      <td>27221</td>\n",
       "      <td>26022</td>\n",
       "      <td>22750</td>\n",
       "      <td>24373</td>\n",
       "      <td>...</td>\n",
       "      <td>22963</td>\n",
       "      <td>22982</td>\n",
       "      <td>23832</td>\n",
       "      <td>27356</td>\n",
       "      <td>27978</td>\n",
       "      <td>25320</td>\n",
       "      <td>25997</td>\n",
       "      <td>20794</td>\n",
       "      <td>23650</td>\n",
       "      <td>20761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>24911</td>\n",
       "      <td>18850</td>\n",
       "      <td>15277</td>\n",
       "      <td>31509</td>\n",
       "      <td>24111</td>\n",
       "      <td>14631</td>\n",
       "      <td>32780</td>\n",
       "      <td>25439</td>\n",
       "      <td>24921</td>\n",
       "      <td>31852</td>\n",
       "      <td>...</td>\n",
       "      <td>26644</td>\n",
       "      <td>15989</td>\n",
       "      <td>27881</td>\n",
       "      <td>29783</td>\n",
       "      <td>35503</td>\n",
       "      <td>20149</td>\n",
       "      <td>31748</td>\n",
       "      <td>14319</td>\n",
       "      <td>21965</td>\n",
       "      <td>13646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>30305</td>\n",
       "      <td>19896</td>\n",
       "      <td>17475</td>\n",
       "      <td>27169</td>\n",
       "      <td>24621</td>\n",
       "      <td>19479</td>\n",
       "      <td>28976</td>\n",
       "      <td>25431</td>\n",
       "      <td>21411</td>\n",
       "      <td>25508</td>\n",
       "      <td>...</td>\n",
       "      <td>22142</td>\n",
       "      <td>21439</td>\n",
       "      <td>25489</td>\n",
       "      <td>27769</td>\n",
       "      <td>28427</td>\n",
       "      <td>23411</td>\n",
       "      <td>28762</td>\n",
       "      <td>20249</td>\n",
       "      <td>23769</td>\n",
       "      <td>19028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>24708</td>\n",
       "      <td>19475</td>\n",
       "      <td>14808</td>\n",
       "      <td>30700</td>\n",
       "      <td>24200</td>\n",
       "      <td>13050</td>\n",
       "      <td>32969</td>\n",
       "      <td>25802</td>\n",
       "      <td>25124</td>\n",
       "      <td>35443</td>\n",
       "      <td>...</td>\n",
       "      <td>27815</td>\n",
       "      <td>12302</td>\n",
       "      <td>30060</td>\n",
       "      <td>28848</td>\n",
       "      <td>32432</td>\n",
       "      <td>17082</td>\n",
       "      <td>33261</td>\n",
       "      <td>12808</td>\n",
       "      <td>19800</td>\n",
       "      <td>10863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>23198</td>\n",
       "      <td>19405</td>\n",
       "      <td>18586</td>\n",
       "      <td>28990</td>\n",
       "      <td>22178</td>\n",
       "      <td>19002</td>\n",
       "      <td>27667</td>\n",
       "      <td>24762</td>\n",
       "      <td>22490</td>\n",
       "      <td>23907</td>\n",
       "      <td>...</td>\n",
       "      <td>22955</td>\n",
       "      <td>22228</td>\n",
       "      <td>22144</td>\n",
       "      <td>28058</td>\n",
       "      <td>27800</td>\n",
       "      <td>23034</td>\n",
       "      <td>26813</td>\n",
       "      <td>18922</td>\n",
       "      <td>25178</td>\n",
       "      <td>19531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>25363</td>\n",
       "      <td>19298</td>\n",
       "      <td>14229</td>\n",
       "      <td>27749</td>\n",
       "      <td>24643</td>\n",
       "      <td>18533</td>\n",
       "      <td>30726</td>\n",
       "      <td>25399</td>\n",
       "      <td>22407</td>\n",
       "      <td>25798</td>\n",
       "      <td>...</td>\n",
       "      <td>23882</td>\n",
       "      <td>21619</td>\n",
       "      <td>26485</td>\n",
       "      <td>27245</td>\n",
       "      <td>30213</td>\n",
       "      <td>23629</td>\n",
       "      <td>27564</td>\n",
       "      <td>18959</td>\n",
       "      <td>23889</td>\n",
       "      <td>18120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>23313</td>\n",
       "      <td>17962</td>\n",
       "      <td>20911</td>\n",
       "      <td>28225</td>\n",
       "      <td>22431</td>\n",
       "      <td>22379</td>\n",
       "      <td>24562</td>\n",
       "      <td>24979</td>\n",
       "      <td>21127</td>\n",
       "      <td>17874</td>\n",
       "      <td>...</td>\n",
       "      <td>21176</td>\n",
       "      <td>27829</td>\n",
       "      <td>20835</td>\n",
       "      <td>25661</td>\n",
       "      <td>24325</td>\n",
       "      <td>26487</td>\n",
       "      <td>22842</td>\n",
       "      <td>23073</td>\n",
       "      <td>27383</td>\n",
       "      <td>23840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>20497</td>\n",
       "      <td>22452</td>\n",
       "      <td>21111</td>\n",
       "      <td>24787</td>\n",
       "      <td>23605</td>\n",
       "      <td>18883</td>\n",
       "      <td>25562</td>\n",
       "      <td>22343</td>\n",
       "      <td>27065</td>\n",
       "      <td>29898</td>\n",
       "      <td>...</td>\n",
       "      <td>45192</td>\n",
       "      <td>16955</td>\n",
       "      <td>26243</td>\n",
       "      <td>23545</td>\n",
       "      <td>26195</td>\n",
       "      <td>19393</td>\n",
       "      <td>23340</td>\n",
       "      <td>17719</td>\n",
       "      <td>20133</td>\n",
       "      <td>17476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>22608</td>\n",
       "      <td>22721</td>\n",
       "      <td>26936</td>\n",
       "      <td>19150</td>\n",
       "      <td>22028</td>\n",
       "      <td>30978</td>\n",
       "      <td>16737</td>\n",
       "      <td>25084</td>\n",
       "      <td>19410</td>\n",
       "      <td>8767</td>\n",
       "      <td>...</td>\n",
       "      <td>16955</td>\n",
       "      <td>45192</td>\n",
       "      <td>15924</td>\n",
       "      <td>20760</td>\n",
       "      <td>17540</td>\n",
       "      <td>31888</td>\n",
       "      <td>14261</td>\n",
       "      <td>31472</td>\n",
       "      <td>28904</td>\n",
       "      <td>33689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>24048</td>\n",
       "      <td>20369</td>\n",
       "      <td>18126</td>\n",
       "      <td>25510</td>\n",
       "      <td>25120</td>\n",
       "      <td>18752</td>\n",
       "      <td>28301</td>\n",
       "      <td>23952</td>\n",
       "      <td>24076</td>\n",
       "      <td>30943</td>\n",
       "      <td>...</td>\n",
       "      <td>26243</td>\n",
       "      <td>15924</td>\n",
       "      <td>45192</td>\n",
       "      <td>23562</td>\n",
       "      <td>27308</td>\n",
       "      <td>19066</td>\n",
       "      <td>28605</td>\n",
       "      <td>18196</td>\n",
       "      <td>20890</td>\n",
       "      <td>15365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>25222</td>\n",
       "      <td>20497</td>\n",
       "      <td>17788</td>\n",
       "      <td>28990</td>\n",
       "      <td>23018</td>\n",
       "      <td>17876</td>\n",
       "      <td>29099</td>\n",
       "      <td>25202</td>\n",
       "      <td>23112</td>\n",
       "      <td>25981</td>\n",
       "      <td>...</td>\n",
       "      <td>23545</td>\n",
       "      <td>20760</td>\n",
       "      <td>23562</td>\n",
       "      <td>45192</td>\n",
       "      <td>29434</td>\n",
       "      <td>22242</td>\n",
       "      <td>28117</td>\n",
       "      <td>18366</td>\n",
       "      <td>23346</td>\n",
       "      <td>18515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>24830</td>\n",
       "      <td>18505</td>\n",
       "      <td>14822</td>\n",
       "      <td>31174</td>\n",
       "      <td>24006</td>\n",
       "      <td>15364</td>\n",
       "      <td>32965</td>\n",
       "      <td>25450</td>\n",
       "      <td>24430</td>\n",
       "      <td>30335</td>\n",
       "      <td>...</td>\n",
       "      <td>26195</td>\n",
       "      <td>17540</td>\n",
       "      <td>27308</td>\n",
       "      <td>29434</td>\n",
       "      <td>45192</td>\n",
       "      <td>20806</td>\n",
       "      <td>30597</td>\n",
       "      <td>14728</td>\n",
       "      <td>22776</td>\n",
       "      <td>14729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>23622</td>\n",
       "      <td>22437</td>\n",
       "      <td>24678</td>\n",
       "      <td>21728</td>\n",
       "      <td>23108</td>\n",
       "      <td>28548</td>\n",
       "      <td>20305</td>\n",
       "      <td>24624</td>\n",
       "      <td>20528</td>\n",
       "      <td>14265</td>\n",
       "      <td>...</td>\n",
       "      <td>19393</td>\n",
       "      <td>31888</td>\n",
       "      <td>19066</td>\n",
       "      <td>22242</td>\n",
       "      <td>20806</td>\n",
       "      <td>45192</td>\n",
       "      <td>17801</td>\n",
       "      <td>28388</td>\n",
       "      <td>27104</td>\n",
       "      <td>29465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>26351</td>\n",
       "      <td>19486</td>\n",
       "      <td>15329</td>\n",
       "      <td>29781</td>\n",
       "      <td>23997</td>\n",
       "      <td>14221</td>\n",
       "      <td>31600</td>\n",
       "      <td>23927</td>\n",
       "      <td>23313</td>\n",
       "      <td>32408</td>\n",
       "      <td>...</td>\n",
       "      <td>23340</td>\n",
       "      <td>14261</td>\n",
       "      <td>28605</td>\n",
       "      <td>28117</td>\n",
       "      <td>30597</td>\n",
       "      <td>17801</td>\n",
       "      <td>45192</td>\n",
       "      <td>14783</td>\n",
       "      <td>20955</td>\n",
       "      <td>12688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>22984</td>\n",
       "      <td>26695</td>\n",
       "      <td>29228</td>\n",
       "      <td>15922</td>\n",
       "      <td>22084</td>\n",
       "      <td>34142</td>\n",
       "      <td>14873</td>\n",
       "      <td>22396</td>\n",
       "      <td>19750</td>\n",
       "      <td>12831</td>\n",
       "      <td>...</td>\n",
       "      <td>17719</td>\n",
       "      <td>31472</td>\n",
       "      <td>18196</td>\n",
       "      <td>18366</td>\n",
       "      <td>14728</td>\n",
       "      <td>28388</td>\n",
       "      <td>14783</td>\n",
       "      <td>45192</td>\n",
       "      <td>24688</td>\n",
       "      <td>33209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>23300</td>\n",
       "      <td>17007</td>\n",
       "      <td>22486</td>\n",
       "      <td>24080</td>\n",
       "      <td>22280</td>\n",
       "      <td>24886</td>\n",
       "      <td>22295</td>\n",
       "      <td>24808</td>\n",
       "      <td>20558</td>\n",
       "      <td>16427</td>\n",
       "      <td>...</td>\n",
       "      <td>20133</td>\n",
       "      <td>28904</td>\n",
       "      <td>20890</td>\n",
       "      <td>23346</td>\n",
       "      <td>22776</td>\n",
       "      <td>27104</td>\n",
       "      <td>20955</td>\n",
       "      <td>24688</td>\n",
       "      <td>45192</td>\n",
       "      <td>25699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>21535</td>\n",
       "      <td>26352</td>\n",
       "      <td>30087</td>\n",
       "      <td>16221</td>\n",
       "      <td>21181</td>\n",
       "      <td>32497</td>\n",
       "      <td>13502</td>\n",
       "      <td>22297</td>\n",
       "      <td>20683</td>\n",
       "      <td>10152</td>\n",
       "      <td>...</td>\n",
       "      <td>17476</td>\n",
       "      <td>33689</td>\n",
       "      <td>15365</td>\n",
       "      <td>18515</td>\n",
       "      <td>14729</td>\n",
       "      <td>29465</td>\n",
       "      <td>12688</td>\n",
       "      <td>33209</td>\n",
       "      <td>25699</td>\n",
       "      <td>45192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Questions    1      2      3      4      5      6      7      8      9    \\\n",
       "Questions                                                                  \n",
       "1          45192  21497  20168  24144  23720  21580  25513  24404  18864   \n",
       "2          21497  45192  27329  17387  21923  26467  17718  20911  23281   \n",
       "3          20168  27329  45192  16278  20942  29612  13241  21006  23024   \n",
       "4          24144  17387  16278  45192  23090  16310  30893  24808  23562   \n",
       "5          23720  21923  20942  23090  45192  22366  24555  24108  22790   \n",
       "6          21580  26467  29612  16310  22366  45192  14825  22294  20664   \n",
       "7          25513  17718  13241  30893  24555  14825  45192  24877  23723   \n",
       "8          24404  20911  21006  24808  24108  22294  24877  45192  22086   \n",
       "9          18864  23281  23024  23562  22790  20664  23723  22086  45192   \n",
       "10         23527  22288  17173  27727  24207  13603  30876  22599  27179   \n",
       "11         20747  23890  24253  21687  23237  26011  21482  23269  25041   \n",
       "12         23521  23824  19157  27289  22641  16573  28714  23063  25667   \n",
       "13         23790  26739  27984  17360  22914  32738  16849  23106  21360   \n",
       "14         25595  22604  20435  25981  24857  21651  26790  25439  23193   \n",
       "15         26375  18624  16929  28623  23747  18167  29132  25037  21343   \n",
       "16         24239  17924  19357  25139  25305  21961  26948  24801  21997   \n",
       "17         24429  19314  16095  31309  22597  15359  31582  25097  24559   \n",
       "18         24692  22193  18154  27692  23828  17886  29271  24672  23910   \n",
       "19         24704  21227  19002  26358  23732  21452  26875  25472  22278   \n",
       "20         26190  22125  19780  25934  22770  19538  27483  23398  23282   \n",
       "21         24501  21580  16461  28435  24281  13781  31700  23441  25951   \n",
       "22         21825  27342  27693  17167  22981  28263  16932  21945  21397   \n",
       "23         23809  22276  21441  24015  23923  22413  24552  24355  23553   \n",
       "24         22786  24437  21682  25126  21748  18864  24645  23308  24472   \n",
       "25         24121  18914  18313  27651  23639  18679  27738  26839  22675   \n",
       "26         24201  23188  21979  23913  21277  20415  24128  22617  22751   \n",
       "27         24265  16668  15907  29643  24563  14601  31266  24279  24349   \n",
       "28         28756  21939  20042  23464  23974  23754  25135  24600  20974   \n",
       "29         22272  22729  20632  26104  22770  18178  26695  22460  25558   \n",
       "30         25736  17603  13376  30534  24620  13872  32903  25042  23272   \n",
       "...          ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "71         21939  23750  22085  21839  25839  22775  22954  22835  23951   \n",
       "72         26748  19535  17492  28112  23722  18986  28679  24606  21160   \n",
       "73         24234  21693  20568  26304  23424  21818  25967  24672  23418   \n",
       "74         22177  30570  25969  20669  21169  26467  18856  22889  21417   \n",
       "75         27088  23133  21212  24566  23134  22032  24321  23516  18334   \n",
       "76         23849  19442  18333  28445  22385  21033  27618  25197  21781   \n",
       "77         24609  19656  16519  30471  23553  15513  31322  24981  24991   \n",
       "78         24754  22925  21368  23944  23234  21924  24429  23304  21914   \n",
       "79         25668  19749  15374  28758  25418  16680  31907  26234  23524   \n",
       "80         23726  16299  17802  26226  24604  19450  28285  24870  22674   \n",
       "81         24250  18697  16342  31074  23874  16240  30941  25238  24910   \n",
       "82         20690  22513  22360  23132  22894  21426  23249  22698  23884   \n",
       "83         25270  24587  22082  22988  23780  24326  24127  23378  22280   \n",
       "84         25346  21177  19038  27160  23664  20352  27221  26022  22750   \n",
       "85         24911  18850  15277  31509  24111  14631  32780  25439  24921   \n",
       "86         30305  19896  17475  27169  24621  19479  28976  25431  21411   \n",
       "87         24708  19475  14808  30700  24200  13050  32969  25802  25124   \n",
       "88         23198  19405  18586  28990  22178  19002  27667  24762  22490   \n",
       "89         25363  19298  14229  27749  24643  18533  30726  25399  22407   \n",
       "90         23313  17962  20911  28225  22431  22379  24562  24979  21127   \n",
       "91         20497  22452  21111  24787  23605  18883  25562  22343  27065   \n",
       "92         22608  22721  26936  19150  22028  30978  16737  25084  19410   \n",
       "93         24048  20369  18126  25510  25120  18752  28301  23952  24076   \n",
       "94         25222  20497  17788  28990  23018  17876  29099  25202  23112   \n",
       "95         24830  18505  14822  31174  24006  15364  32965  25450  24430   \n",
       "96         23622  22437  24678  21728  23108  28548  20305  24624  20528   \n",
       "97         26351  19486  15329  29781  23997  14221  31600  23927  23313   \n",
       "98         22984  26695  29228  15922  22084  34142  14873  22396  19750   \n",
       "99         23300  17007  22486  24080  22280  24886  22295  24808  20558   \n",
       "100        21535  26352  30087  16221  21181  32497  13502  22297  20683   \n",
       "\n",
       "Questions    10   ...      91     92     93     94     95     96     97   \\\n",
       "Questions         ...                                                      \n",
       "1          23527  ...    20497  22608  24048  25222  24830  23622  26351   \n",
       "2          22288  ...    22452  22721  20369  20497  18505  22437  19486   \n",
       "3          17173  ...    21111  26936  18126  17788  14822  24678  15329   \n",
       "4          27727  ...    24787  19150  25510  28990  31174  21728  29781   \n",
       "5          24207  ...    23605  22028  25120  23018  24006  23108  23997   \n",
       "6          13603  ...    18883  30978  18752  17876  15364  28548  14221   \n",
       "7          30876  ...    25562  16737  28301  29099  32965  20305  31600   \n",
       "8          22599  ...    22343  25084  23952  25202  25450  24624  23927   \n",
       "9          27179  ...    27065  19410  24076  23112  24430  20528  23313   \n",
       "10         45192  ...    29898   8767  30943  25981  30335  14265  32408   \n",
       "11         22642  ...    24658  23299  24303  22259  22789  23493  20778   \n",
       "12         33988  ...    27472  13025  27767  26869  28849  17607  29974   \n",
       "13         18597  ...    20807  26620  21108  19444  17340  24986  17531   \n",
       "14         27192  ...    24070  20261  26097  25609  27135  23891  26594   \n",
       "15         24806  ...    20590  21149  25395  27267  28299  22621  32446   \n",
       "16         23030  ...    22968  23311  27803  23743  26103  24677  24656   \n",
       "17         30108  ...    25504  17323  26303  31415  31995  19565  30474   \n",
       "18         29603  ...    24509  17528  26220  27690  28810  20410  29405   \n",
       "19         22733  ...    22495  24188  23666  26798  27406  26608  24373   \n",
       "20         29661  ...    24793  16564  26440  25666  26326  19450  29055   \n",
       "21         37918  ...    29164   9793  30959  27647  31047  14575  33246   \n",
       "22         17932  ...    21046  26679  20553  19257  16723  24787  15902   \n",
       "23         22720  ...    22816  24395  22295  25193  24999  23393  23284   \n",
       "24         27779  ...    24837  18586  20196  26716  25490  19068  25747   \n",
       "25         24174  ...    23094  22443  24231  28081  27923  22761  26554   \n",
       "26         25820  ...    23946  19833  23379  24497  24141  21217  24656   \n",
       "27         32560  ...    26898  14371  29727  26815  30127  18021  31274   \n",
       "28         22305  ...    21633  24152  23674  24848  24636  24458  23783   \n",
       "29         32041  ...    28013  14746  27400  24742  27058  18332  27553   \n",
       "30         29567  ...    24811  17198  27848  28914  31746  20144  32019   \n",
       "...          ...  ...      ...    ...    ...    ...    ...    ...    ...   \n",
       "71         26472  ...    24820  19347  26067  21033  22839  20387  23786   \n",
       "72         23313  ...    20359  22778  23686  28008  28100  24014  29419   \n",
       "73         22675  ...    22423  24292  22650  27650  26252  24394  24481   \n",
       "74         18138  ...    20200  27201  17527  22865  19657  25047  19118   \n",
       "75         22221  ...    20271  22638  23084  24742  23028  23134  25169   \n",
       "76         21170  ...    21730  25671  21445  28177  27823  25847  25250   \n",
       "77         30736  ...    26098  16965  27159  29885  31341  19485  30004   \n",
       "78         24913  ...    16845  20574  24238  24350  23858  21672  28015   \n",
       "79         29875  ...    25223  17998  28320  27878  31376  21458  30145   \n",
       "80         25889  ...    24427  20430  29640  23908  27132  22302  26791   \n",
       "81         28947  ...    26141  18570  26524  28804  34076  21824  29027   \n",
       "82         23139  ...    23929  22198  22616  23126  23270  21730  22135   \n",
       "83         25389  ...    22737  21032  24834  22856  23212  23364  25133   \n",
       "84         24373  ...    22963  22982  23832  27356  27978  25320  25997   \n",
       "85         31852  ...    26644  15989  27881  29783  35503  20149  31748   \n",
       "86         25508  ...    22142  21439  25489  27769  28427  23411  28762   \n",
       "87         35443  ...    27815  12302  30060  28848  32432  17082  33261   \n",
       "88         23907  ...    22955  22228  22144  28058  27800  23034  26813   \n",
       "89         25798  ...    23882  21619  26485  27245  30213  23629  27564   \n",
       "90         17874  ...    21176  27829  20835  25661  24325  26487  22842   \n",
       "91         29898  ...    45192  16955  26243  23545  26195  19393  23340   \n",
       "92          8767  ...    16955  45192  15924  20760  17540  31888  14261   \n",
       "93         30943  ...    26243  15924  45192  23562  27308  19066  28605   \n",
       "94         25981  ...    23545  20760  23562  45192  29434  22242  28117   \n",
       "95         30335  ...    26195  17540  27308  29434  45192  20806  30597   \n",
       "96         14265  ...    19393  31888  19066  22242  20806  45192  17801   \n",
       "97         32408  ...    23340  14261  28605  28117  30597  17801  45192   \n",
       "98         12831  ...    17719  31472  18196  18366  14728  28388  14783   \n",
       "99         16427  ...    20133  28904  20890  23346  22776  27104  20955   \n",
       "100        10152  ...    17476  33689  15365  18515  14729  29465  12688   \n",
       "\n",
       "Questions    98     99     100  \n",
       "Questions                       \n",
       "1          22984  23300  21535  \n",
       "2          26695  17007  26352  \n",
       "3          29228  22486  30087  \n",
       "4          15922  24080  16221  \n",
       "5          22084  22280  21181  \n",
       "6          34142  24886  32497  \n",
       "7          14873  22295  13502  \n",
       "8          22396  24808  22297  \n",
       "9          19750  20558  20683  \n",
       "10         12831  16427  10152  \n",
       "11         23901  22733  24132  \n",
       "12         16069  18475  14404  \n",
       "13         31844  22232  29011  \n",
       "14         21251  21833  19674  \n",
       "15         18917  24727  18070  \n",
       "16         21353  25787  20548  \n",
       "17         15265  22525  15178  \n",
       "18         18030  20892  16923  \n",
       "19         21548  24164  21939  \n",
       "20         19480  20710  17621  \n",
       "21         13095  17323  10342  \n",
       "22         29047  21215  28966  \n",
       "23         22701  23231  23366  \n",
       "24         19480  19550  19417  \n",
       "25         19265  25265  19578  \n",
       "26         21035  21385  20390  \n",
       "27         14103  21567  12178  \n",
       "28         24476  23698  22991  \n",
       "29         16826  19228  15789  \n",
       "30         14360  22958  13877  \n",
       "...          ...    ...    ...  \n",
       "71         21607  19993  20278  \n",
       "72         20226  24736  19819  \n",
       "73         21824  23772  22471  \n",
       "74         27461  21765  28080  \n",
       "75         24098  22394  22399  \n",
       "76         20927  26471  21682  \n",
       "77         15283  21939  15088  \n",
       "78         22436  22012  21121  \n",
       "79         17002  21880  15321  \n",
       "80         18914  25350  17449  \n",
       "81         15342  23460  16239  \n",
       "82         21192  22506  21475  \n",
       "83         25576  20954  22091  \n",
       "84         20794  23650  20761  \n",
       "85         14319  21965  13646  \n",
       "86         20249  23769  19028  \n",
       "87         12808  19800  10863  \n",
       "88         18922  25178  19531  \n",
       "89         18959  23889  18120  \n",
       "90         23073  27383  23840  \n",
       "91         17719  20133  17476  \n",
       "92         31472  28904  33689  \n",
       "93         18196  20890  15365  \n",
       "94         18366  23346  18515  \n",
       "95         14728  22776  14729  \n",
       "96         28388  27104  29465  \n",
       "97         14783  20955  12688  \n",
       "98         45192  24688  33209  \n",
       "99         24688  45192  25699  \n",
       "100        33209  25699  45192  \n",
       "\n",
       "[100 rows x 100 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##for show\n",
    "##ratings\n",
    "##array_pd\n",
    "##cross\n",
    "##QbyQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [3/5] dirty .txt file preprocessed to pretty .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratings = ratings.drop('2015-11-16.1', axis=1)\n",
    "ratings = ratings.drop('20:50:07', axis=1)\n",
    "ratings = ratings.rename(index=str, columns={\"U9\": \"Users_ID\", \"2015-11-16\": \"Questions\", \"20:44:38\" : \"Y_or_N\"})\n",
    "ratings = ratings[(ratings.Y_or_N == 'Y') | (ratings.Y_or_N == 'N')]\n",
    "\n",
    "ratings['Users_ID'] = ratings['Users_ID'].str.replace(r'\\D+', '').astype('int')\n",
    "ratings['Questions'] = ratings['Questions'].str.replace(r'\\D+', '').astype('int')\n",
    "ratings['Y_or_N'] = ratings['Y_or_N'].str.replace('N', '0')\n",
    "ratings['Y_or_N'] = ratings['Y_or_N'].str.replace('Y', '1')\n",
    "ratings['Y_or_N'] = ratings['Y_or_N'].str.replace(r'\\D+', '').astype('int')\n",
    "\n",
    "ratings = ratings.sort(['Questions','Users_ID'], ascending=[True, True])\n",
    "\n",
    "ratings.to_pickle(model_path + 'ratings.pkl')\n",
    "ratings = pd.read_pickle(model_path + 'ratings.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [4/5] Crosstab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross = pd.crosstab(ratings.Users_ID, ratings.Questions, ratings.Y_or_N , aggfunc=np.sum)\n",
    "cross.to_pickle(model_path + 'cross.pkl')\n",
    "cross = pd.read_pickle(model_path + 'cross.pkl')\n",
    "\n",
    "cross_array = cross.as_matrix()\n",
    "cross_array.as.savetxt(model_path + 'cross_array.pkl')\n",
    "\n",
    "users = ratings.Users_ID.unique()\n",
    "questions = ratings.Questions.unique()\n",
    "\n",
    "array_pd = pd.DataFrame(cross_array, index=users, columns=questions)\n",
    "array_pd.to_pickle(model_path + 'array_pd.pkl')\n",
    "array_pd = pd.read_pickle(model_path + 'array_pd.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5/5] QbyQ and UbyU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions_by_Questions = cross.transpose().dot(cross)\n",
    "Questions_by_Questions_not = cross_not.transpose().dot(cross_not)\n",
    "Questions_by_Questions_not.to_pickle(model_path +'Questions_by_Questions_not.pkl')\n",
    "Questions_by_Questions.to_pickle(model_path + 'Questions_by_Questions.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "user_by_user_not = cross_not.dot(cross_not.transpose())\n",
    "unit1 = user_by_user.loc[:4000]\n",
    "unit2 = user_by_user.loc[4001:8000]\n",
    "unit3 = user_by_user.loc[8001:12000]\n",
    "unit4 = user_by_user.loc[12001:16000]\n",
    "unit5 = user_by_user.loc[16001:20000]\n",
    "unit6 = user_by_user.loc[20001:24000]\n",
    "unit7 = user_by_user.loc[24001:28000]\n",
    "unit8 = user_by_user.loc[28001:32000]\n",
    "unit9 = user_by_user.loc[32001:36000]\n",
    "unit10 = user_by_user.loc[36001:40000]\n",
    "unit11 = user_by_user.loc[40001:44000]\n",
    "unit12 = user_by_user.loc[44001:48000]\n",
    "unit13 = user_by_user.loc[48001:52000]\n",
    "unit14 = user_by_user.loc[52001:]\n",
    "unit1.to_pickle(model_path + 'user_by_user_u1.pkl')\n",
    "unit2.to_pickle(model_path + 'user_by_user_u2.pkl')\n",
    "unit3.to_pickle(model_path + 'user_by_user_u3.pkl')\n",
    "unit4.to_pickle(model_path + 'user_by_user_u4.pkl')\n",
    "unit5.to_pickle(model_path + 'user_by_user_u5.pkl')\n",
    "unit6.to_pickle(model_path + 'user_by_user_u6.pkl')\n",
    "unit7.to_pickle(model_path + 'user_by_user_u7.pkl')\n",
    "unit8.to_pickle(model_path + 'user_by_user_u8.pkl')\n",
    "unit9.to_pickle(model_path + 'user_by_user_u9.pkl')\n",
    "unit10.to_pickle(model_path + 'user_by_user_u10.pkl')\n",
    "unit11.to_pickle(model_path + 'user_by_user_u11.pkl')\n",
    "unit12.to_pickle(model_path + 'user_by_user_u12.pkl')\n",
    "unit13.to_pickle(model_path + 'user_by_user_u13.pkl')\n",
    "unit14.to_pickle(model_path + 'user_by_user_u14.pkl')\n",
    "\n",
    "user_by_user_not = cross_not.dot(cross_not.transpose())\n",
    "not_unit1 = user_by_user_not.loc[:4000]\n",
    "not_unit2 = user_by_user_not.loc[4001:8000]\n",
    "not_unit3 = user_by_user_not.loc[8001:12000]\n",
    "not_unit4 = user_by_user_not.loc[12001:16000]\n",
    "not_unit5 = user_by_user_not.loc[16001:20000]\n",
    "not_unit6 = user_by_user_not.loc[20001:24000]\n",
    "not_unit7 = user_by_user_not.loc[24001:28000]\n",
    "not_unit8 = user_by_user_not.loc[28001:32000]\n",
    "not_unit9 = user_by_user_not.loc[32001:36000]\n",
    "not_unit10 = user_by_user_not.loc[36001:40000]\n",
    "not_unit11 = user_by_user_not.loc[40001:44000]\n",
    "not_unit12 = user_by_user_not.loc[44001:48000]\n",
    "not_unit13 = user_by_user_not.loc[48001:52000]\n",
    "not_unit14 = user_by_user_not.loc[52001:]\n",
    "\n",
    "not_unit1.to_pickle(model_path + 'user_by_user_not_u1.pkl')\n",
    "not_unit2.to_pickle(model_path + 'user_by_user_not_u2.pkl')\n",
    "not_unit3.to_pickle(model_path + 'user_by_user_not_u3.pkl')\n",
    "not_unit4.to_pickle(model_path + 'user_by_user_not_u4.pkl')\n",
    "not_unit5.to_pickle(model_path + 'user_by_user_not_u5.pkl')\n",
    "not_unit6.to_pickle(model_path + 'user_by_user_not_u6.pkl')\n",
    "not_unit7.to_pickle(model_path + 'user_by_user_not_u7.pkl')\n",
    "not_unit8.to_pickle(model_path + 'user_by_user_not_u8.pkl')\n",
    "not_unit9.to_pickle(model_path + 'user_by_user_not_u9.pkl')\n",
    "not_unit10.to_pickle(model_path + 'user_by_user_not_u10.pkl')\n",
    "not_unit11.to_pickle(model_path + 'user_by_user_not_u11.pkl')\n",
    "not_unit12.to_pickle(model_path + 'user_by_user_not_u12.pkl')\n",
    "not_unit13.to_pickle(model_path + 'user_by_user_not_u13.pkl')\n",
    "not_unit14.to_pickle(model_path + 'user_by_user_not_u14.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Split\n",
    "training set, validation set, test set을 대략 60, 20, 20으로 구분했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(ratings)) < 0.9\n",
    "trn_and_test = ratings[msk]\n",
    "val = ratings[~msk]\n",
    "\n",
    "msk2 = np.random.rand(len(trn_and_test)) < 0.9\n",
    "trn = trn_and_test[msk2]\n",
    "test = trn_and_test[~msk2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4519200"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the dataframe\n",
    "ratings.sort(columns=['Users_ID'], inplace=True)\n",
    "# set the index to be this and don't drop\n",
    "ratings.set_index(keys=['Users_ID'], drop=False,inplace=True)\n",
    "# get a list of names\n",
    "names=ratings['Users_ID'].unique().tolist()\n",
    "\n",
    "# now we can perform a lookup on a 'view' of the dataframe\n",
    "# now you can query all 'joes'\n",
    "trn = ratings.loc[(ratings.Users_ID>=1) & (ratings.Users_ID<=32000)]\n",
    "val = ratings.loc[(ratings.Users_ID>32000) & (ratings.Users_ID<=44000)]\n",
    "test = ratings.loc[(ratings.Users_ID>44000)]\n",
    "len(test) + len(trn) + len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = trn.sort('Users_ID', ascending= True)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "categorical하게 풀 수 있도록 yes no값을 [0,1] [1,0] 으로 바꾸었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_category = to_categorical(trn.Y_or_N)\n",
    "val_category = to_categorical(val.Y_or_N)\n",
    "test_category = to_categorical(test.Y_or_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Questions</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "      <th>100</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Users_ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54069</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54070</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54071</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54073</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54075</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54077</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54078</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54079</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54080</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54081</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54083</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54084</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54085</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54088</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54089</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54090</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54091</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54092</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54093</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54094</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54095</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54096</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54097</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54098</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54100</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54101</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54102</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54103</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54104</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54106</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45192 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Questions  1    2    3    4    5    6    7    8    9    10  ...   91   92   \\\n",
       "Users_ID                                                    ...              \n",
       "9          1.0  0.0  NaN  1.0  1.0  NaN  1.0  1.0  1.0  NaN ...   1.0  NaN   \n",
       "13         NaN  NaN  1.0  0.0  1.0  NaN  0.0  0.0  1.0  0.0 ...   0.0  NaN   \n",
       "14         0.0  0.0  0.0  0.0  0.0  NaN  0.0  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "15         NaN  0.0  NaN  0.0  1.0  1.0  1.0  1.0  0.0  0.0 ...   NaN  NaN   \n",
       "16         NaN  0.0  1.0  1.0  0.0  NaN  0.0  1.0  0.0  NaN ...   0.0  1.0   \n",
       "17         1.0  NaN  0.0  0.0  0.0  1.0  1.0  1.0  0.0  NaN ...   1.0  1.0   \n",
       "20         0.0  1.0  1.0  0.0  0.0  NaN  1.0  1.0  1.0  1.0 ...   0.0  0.0   \n",
       "21         1.0  1.0  NaN  0.0  NaN  1.0  0.0  1.0  1.0  1.0 ...   1.0  NaN   \n",
       "22         0.0  0.0  NaN  0.0  0.0  0.0  0.0  NaN  0.0  0.0 ...   0.0  0.0   \n",
       "26         NaN  1.0  NaN  0.0  0.0  1.0  1.0  1.0  0.0  1.0 ...   0.0  NaN   \n",
       "27         0.0  0.0  1.0  1.0  0.0  0.0  NaN  1.0  0.0  1.0 ...   1.0  0.0   \n",
       "29         1.0  1.0  1.0  0.0  0.0  1.0  0.0  1.0  0.0  NaN ...   0.0  0.0   \n",
       "30         1.0  NaN  NaN  0.0  NaN  0.0  0.0  NaN  0.0  1.0 ...   1.0  0.0   \n",
       "31         0.0  1.0  NaN  0.0  0.0  1.0  0.0  1.0  NaN  1.0 ...   1.0  0.0   \n",
       "32         NaN  1.0  1.0  1.0  0.0  1.0  0.0  NaN  0.0  1.0 ...   1.0  0.0   \n",
       "34         NaN  0.0  NaN  1.0  0.0  0.0  0.0  1.0  NaN  NaN ...   1.0  0.0   \n",
       "35         NaN  1.0  NaN  0.0  NaN  0.0  0.0  1.0  1.0  1.0 ...   1.0  0.0   \n",
       "36         1.0  0.0  1.0  0.0  1.0  0.0  1.0  NaN  1.0  1.0 ...   0.0  0.0   \n",
       "37         1.0  NaN  NaN  0.0  0.0  NaN  1.0  1.0  1.0  1.0 ...   1.0  0.0   \n",
       "39         0.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0  1.0  1.0 ...   1.0  0.0   \n",
       "40         0.0  NaN  NaN  0.0  0.0  1.0  1.0  1.0  1.0  1.0 ...   1.0  0.0   \n",
       "42         0.0  NaN  0.0  NaN  1.0  0.0  1.0  1.0  1.0  1.0 ...   0.0  1.0   \n",
       "43         1.0  0.0  1.0  1.0  0.0  NaN  1.0  1.0  0.0  NaN ...   NaN  0.0   \n",
       "44         1.0  1.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  NaN ...   1.0  0.0   \n",
       "49         1.0  0.0  1.0  NaN  1.0  NaN  1.0  1.0  0.0  1.0 ...   NaN  0.0   \n",
       "50         0.0  1.0  NaN  1.0  1.0  1.0  0.0  0.0  NaN  1.0 ...   1.0  0.0   \n",
       "51         1.0  NaN  0.0  NaN  0.0  0.0  1.0  1.0  1.0  0.0 ...   0.0  0.0   \n",
       "52         NaN  1.0  1.0  0.0  0.0  1.0  0.0  0.0  1.0  1.0 ...   0.0  0.0   \n",
       "53         1.0  1.0  0.0  1.0  1.0  0.0  1.0  0.0  1.0  1.0 ...   1.0  1.0   \n",
       "54         0.0  1.0  0.0  0.0  0.0  1.0  1.0  0.0  1.0  1.0 ...   NaN  0.0   \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...  ...  ... ...   ...  ...   \n",
       "54069      NaN  0.0  0.0  1.0  1.0  0.0  1.0  NaN  1.0  NaN ...   NaN  1.0   \n",
       "54070      0.0  1.0  NaN  1.0  NaN  0.0  1.0  0.0  NaN  1.0 ...   1.0  0.0   \n",
       "54071      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  1.0 ...   1.0  0.0   \n",
       "54073      1.0  0.0  NaN  1.0  1.0  1.0  1.0  NaN  1.0  0.0 ...   1.0  1.0   \n",
       "54075      1.0  1.0  0.0  0.0  NaN  1.0  0.0  0.0  1.0  NaN ...   0.0  0.0   \n",
       "54077      NaN  1.0  0.0  NaN  1.0  1.0  1.0  0.0  NaN  1.0 ...   1.0  NaN   \n",
       "54078      1.0  1.0  1.0  1.0  NaN  1.0  1.0  1.0  1.0  1.0 ...   0.0  NaN   \n",
       "54079      1.0  1.0  1.0  NaN  NaN  NaN  0.0  0.0  NaN  1.0 ...   0.0  0.0   \n",
       "54080      NaN  1.0  NaN  0.0  1.0  0.0  0.0  0.0  0.0  NaN ...   1.0  NaN   \n",
       "54081      1.0  NaN  0.0  1.0  0.0  NaN  1.0  0.0  0.0  1.0 ...   NaN  0.0   \n",
       "54083      0.0  1.0  0.0  1.0  NaN  1.0  NaN  1.0  NaN  NaN ...   1.0  NaN   \n",
       "54084      1.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  0.0  NaN ...   1.0  0.0   \n",
       "54085      1.0  0.0  0.0  1.0  0.0  NaN  1.0  1.0  0.0  0.0 ...   1.0  0.0   \n",
       "54088      0.0  0.0  1.0  1.0  1.0  1.0  1.0  0.0  1.0  1.0 ...   0.0  0.0   \n",
       "54089      1.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0  NaN  NaN ...   1.0  NaN   \n",
       "54090      1.0  1.0  0.0  1.0  NaN  1.0  1.0  1.0  1.0  NaN ...   NaN  0.0   \n",
       "54091      1.0  NaN  0.0  1.0  1.0  NaN  1.0  1.0  1.0  NaN ...   1.0  0.0   \n",
       "54092      0.0  1.0  0.0  1.0  0.0  0.0  NaN  0.0  0.0  1.0 ...   1.0  0.0   \n",
       "54093      1.0  NaN  0.0  0.0  1.0  0.0  1.0  0.0  NaN  1.0 ...   0.0  0.0   \n",
       "54094      0.0  1.0  1.0  0.0  0.0  1.0  1.0  0.0  1.0  1.0 ...   NaN  0.0   \n",
       "54095      1.0  1.0  0.0  NaN  NaN  1.0  0.0  0.0  NaN  1.0 ...   NaN  NaN   \n",
       "54096      1.0  0.0  0.0  1.0  1.0  0.0  1.0  0.0  0.0  NaN ...   1.0  0.0   \n",
       "54097      0.0  0.0  NaN  1.0  1.0  1.0  1.0  NaN  0.0  0.0 ...   0.0  0.0   \n",
       "54098      NaN  0.0  0.0  NaN  NaN  1.0  1.0  0.0  1.0  1.0 ...   1.0  0.0   \n",
       "54100      0.0  1.0  1.0  1.0  0.0  NaN  1.0  0.0  1.0  1.0 ...   1.0  0.0   \n",
       "54101      0.0  NaN  1.0  1.0  0.0  1.0  NaN  0.0  1.0  1.0 ...   1.0  0.0   \n",
       "54102      1.0  0.0  NaN  1.0  0.0  NaN  1.0  NaN  0.0  0.0 ...   1.0  0.0   \n",
       "54103      0.0  1.0  1.0  0.0  1.0  0.0  1.0  NaN  NaN  NaN ...   1.0  0.0   \n",
       "54104      1.0  NaN  NaN  NaN  1.0  0.0  1.0  1.0  0.0  1.0 ...   1.0  NaN   \n",
       "54106      NaN  NaN  1.0  0.0  NaN  1.0  0.0  0.0  0.0  0.0 ...   0.0  0.0   \n",
       "\n",
       "Questions  93   94   95   96   97   98   99   100  \n",
       "Users_ID                                           \n",
       "9          1.0  1.0  1.0  0.0  1.0  0.0  1.0  0.0  \n",
       "13         1.0  0.0  0.0  NaN  0.0  1.0  1.0  0.0  \n",
       "14         0.0  0.0  0.0  0.0  NaN  0.0  0.0  0.0  \n",
       "15         0.0  1.0  0.0  NaN  1.0  1.0  1.0  0.0  \n",
       "16         1.0  NaN  1.0  1.0  NaN  0.0  1.0  1.0  \n",
       "17         1.0  0.0  1.0  NaN  0.0  0.0  0.0  0.0  \n",
       "20         1.0  1.0  1.0  0.0  NaN  0.0  NaN  0.0  \n",
       "21         1.0  1.0  0.0  0.0  0.0  0.0  1.0  0.0  \n",
       "22         NaN  0.0  1.0  0.0  0.0  NaN  0.0  0.0  \n",
       "26         NaN  0.0  0.0  0.0  NaN  1.0  0.0  1.0  \n",
       "27         0.0  1.0  NaN  0.0  NaN  0.0  1.0  NaN  \n",
       "29         0.0  1.0  0.0  NaN  1.0  NaN  0.0  0.0  \n",
       "30         NaN  0.0  0.0  0.0  NaN  1.0  0.0  0.0  \n",
       "31         1.0  0.0  NaN  NaN  0.0  1.0  0.0  0.0  \n",
       "32         1.0  1.0  1.0  0.0  0.0  0.0  0.0  0.0  \n",
       "34         1.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  \n",
       "35         0.0  0.0  0.0  0.0  NaN  1.0  0.0  0.0  \n",
       "36         1.0  1.0  1.0  0.0  1.0  0.0  0.0  NaN  \n",
       "37         0.0  0.0  1.0  1.0  0.0  0.0  0.0  0.0  \n",
       "39         1.0  1.0  0.0  0.0  0.0  NaN  0.0  NaN  \n",
       "40         1.0  NaN  0.0  0.0  0.0  1.0  0.0  1.0  \n",
       "42         1.0  1.0  NaN  0.0  1.0  0.0  1.0  0.0  \n",
       "43         1.0  0.0  0.0  0.0  1.0  1.0  1.0  NaN  \n",
       "44         NaN  1.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "49         1.0  NaN  1.0  0.0  NaN  0.0  0.0  1.0  \n",
       "50         NaN  1.0  1.0  0.0  1.0  0.0  NaN  1.0  \n",
       "51         1.0  0.0  1.0  0.0  0.0  1.0  0.0  0.0  \n",
       "52         1.0  0.0  0.0  0.0  1.0  0.0  0.0  0.0  \n",
       "53         1.0  NaN  0.0  NaN  1.0  0.0  0.0  NaN  \n",
       "54         0.0  0.0  0.0  0.0  NaN  0.0  1.0  1.0  \n",
       "...        ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "54069      NaN  1.0  NaN  1.0  NaN  0.0  0.0  0.0  \n",
       "54070      1.0  1.0  0.0  1.0  1.0  0.0  1.0  0.0  \n",
       "54071      1.0  0.0  0.0  NaN  0.0  NaN  0.0  NaN  \n",
       "54073      1.0  0.0  1.0  1.0  0.0  0.0  1.0  1.0  \n",
       "54075      1.0  1.0  NaN  NaN  NaN  1.0  0.0  0.0  \n",
       "54077      NaN  NaN  NaN  1.0  0.0  0.0  0.0  0.0  \n",
       "54078      1.0  1.0  1.0  1.0  1.0  1.0  1.0  1.0  \n",
       "54079      0.0  1.0  1.0  0.0  1.0  0.0  NaN  0.0  \n",
       "54080      0.0  0.0  0.0  0.0  1.0  0.0  0.0  NaN  \n",
       "54081      0.0  NaN  1.0  0.0  1.0  0.0  0.0  0.0  \n",
       "54083      1.0  1.0  1.0  1.0  1.0  0.0  NaN  0.0  \n",
       "54084      1.0  1.0  1.0  0.0  0.0  0.0  NaN  0.0  \n",
       "54085      NaN  1.0  1.0  0.0  1.0  0.0  1.0  0.0  \n",
       "54088      0.0  0.0  1.0  1.0  1.0  NaN  1.0  0.0  \n",
       "54089      NaN  1.0  NaN  1.0  0.0  1.0  1.0  0.0  \n",
       "54090      1.0  1.0  0.0  1.0  1.0  1.0  NaN  0.0  \n",
       "54091      0.0  1.0  1.0  1.0  NaN  0.0  1.0  0.0  \n",
       "54092      0.0  NaN  1.0  1.0  0.0  0.0  1.0  0.0  \n",
       "54093      1.0  1.0  0.0  NaN  1.0  NaN  1.0  0.0  \n",
       "54094      NaN  1.0  1.0  NaN  1.0  0.0  0.0  0.0  \n",
       "54095      1.0  0.0  0.0  1.0  1.0  NaN  1.0  0.0  \n",
       "54096      NaN  0.0  1.0  1.0  1.0  0.0  1.0  0.0  \n",
       "54097      0.0  1.0  NaN  1.0  NaN  1.0  1.0  NaN  \n",
       "54098      1.0  1.0  1.0  1.0  1.0  0.0  0.0  NaN  \n",
       "54100      1.0  NaN  1.0  1.0  0.0  0.0  0.0  0.0  \n",
       "54101      1.0  0.0  1.0  1.0  0.0  0.0  1.0  0.0  \n",
       "54102      1.0  1.0  1.0  1.0  1.0  0.0  1.0  0.0  \n",
       "54103      1.0  1.0  NaN  0.0  1.0  0.0  0.0  0.0  \n",
       "54104      1.0  1.0  1.0  NaN  1.0  0.0  1.0  NaN  \n",
       "54106      1.0  0.0  NaN  1.0  NaN  0.0  0.0  NaN  \n",
       "\n",
       "[45192 rows x 100 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_tab = pd.crosstab(trn.Users_ID, trn.Questions, trn.Y_or_N , aggfunc=np.sum)\n",
    "trn_tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) pearson corrleation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix = array_pd.corr(method='pearson')\n",
    "pearson_cor_10_largest = correlation_matrix[1].nlargest(10)\n",
    "\n",
    "for i in range(99) : \n",
    "    pearson_cor_10_largest = pearson_cor_10_largest.append(correlation_matrix[i+2].nlargest(10))\n",
    "\n",
    "sliced_pearson_cor = []\n",
    "\n",
    "for i in range(0, len(pearson_cor_10_largest.as_matrix()), 10):\n",
    "    sliced_pearson_cor.append(pearson_cor_10_largest.as_matrix()[i : i+10])\n",
    "\n",
    "np_sliced_pearson_cor = np.array(sliced_pearson_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_sliced_pearson_cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) kendall's Tau corrleation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kendall_correlation_matrix = array_pd.corr(method='kendall')\n",
    "\n",
    "kendall_cor_10_largest = kendall_correlation_matrix[1].nlargest(10)\n",
    "\n",
    "for i in range(99) : \n",
    "    kendall_cor_10_largest = kendall_cor_10_largest.append(kendall_correlation_matrix[i+2].nlargest(10))\n",
    "\n",
    "sliced_kendall_cor = []\n",
    "\n",
    "for i in range(0, len(kendall_cor_10_largest.as_matrix()), 10):\n",
    "    sliced_pearson_cor.append(kendall_cor_10_largest.as_matrix()[i : i+10])\n",
    "\n",
    "np_sliced_kendall_cor = np.array(sliced_kendall_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np_sliced_kendall_cor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) neighbor based correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "UbyU1 = pd.read_pickle(model_path + 'user_by_user_u1.pkl')\n",
    "UbyUnot1 = pd.read_pickle(model_path + 'user_by_user_not_u1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UbyU = UbyU1+UbyUnot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QbyQ -= np.mean(QbyQ, axis = 1)\n",
    "QbyQ /= np.std(QbyQ, axis = 1)\n",
    "\n",
    "UbyU -= np.mean(UbyU, axis = 1)\n",
    "UbyU /= np.std(UbyU, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QbyQ.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3313, 45192)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UbyU.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4) cosign similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) Classical Dot Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = trn.shape[0]\n",
    "n_questions = 100\n",
    "n_factors = 50\n",
    "\n",
    "user_in = Input(shape=(1,), dtype='int64', name='user_in')\n",
    "u = Embedding(n_users+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(user_in)\n",
    "\n",
    "questions_in = Input(shape=(1,), dtype='int64', name='questions_in')\n",
    "m = Embedding(n_questions+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(questions_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1 = keras.layers.merge([u, m], mode='dot')\n",
    "x1 = Flatten()(x1)\n",
    "x1 = Dense(2)(x1)\n",
    "\n",
    "x1 = Activation('softmax')(x1)\n",
    "model = Model([user_in, questions_in], x1)\n",
    "model.compile(Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, epochs=2, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2640449 samples, validate on 903821 samples\n",
      "Epoch 1/1\n",
      "2640449/2640449 [==============================] - 261s - loss: 1.4457 - acc: 0.5584 - val_loss: 0.9860 - val_acc: 0.5586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12e7fffd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, epochs=1, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2640449 samples, validate on 903821 samples\n",
      "Epoch 1/4\n",
      "2640449/2640449 [==============================] - 207s - loss: 0.8442 - acc: 0.5584 - val_loss: 0.7363 - val_acc: 0.5586\n",
      "Epoch 2/4\n",
      "2640449/2640449 [==============================] - 190s - loss: 0.7112 - acc: 0.5584 - val_loss: 0.6944 - val_acc: 0.5586\n",
      "Epoch 3/4\n",
      "2640449/2640449 [==============================] - 189s - loss: 0.6911 - acc: 0.5584 - val_loss: 0.6888 - val_acc: 0.5586\n",
      "Epoch 4/4\n",
      "2640449/2640449 [==============================] - 207s - loss: 0.6882 - acc: 0.5584 - val_loss: 0.6876 - val_acc: 0.5586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12f055650>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, epochs=4, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "user_in (InputLayer)             (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "questions_in (InputLayer)        (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)          (None, 1, 50)         132022500   user_in[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)          (None, 1, 50)         5050        questions_in[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "merge_2 (Merge)                  (None, 1, 1)          0           embedding_3[0][0]                \n",
      "                                                                   embedding_4[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)              (None, 1)             0           merge_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 2)             4           flatten_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 2)             0           dense_2[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 132,027,554\n",
      "Trainable params: 132,027,554\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "after_softmax = model.predict([test.Users_ID,test.Questions], batch_size = 200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.42808649,  0.57191354],\n",
       "       [ 0.42838645,  0.57161355],\n",
       "       [ 0.42795101,  0.57204902],\n",
       "       ..., \n",
       "       [ 0.42777911,  0.57222092],\n",
       "       [ 0.42817867,  0.57182139],\n",
       "       [ 0.42859548,  0.57140452]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "after_softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       ..., \n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(after_softmax.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate([test.Users_ID,test.Questions], test.Y_or_N, batch_size = 200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) classical neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_users = trn.shape[0]\n",
    "n_questions = 100\n",
    "n_factors = 50\n",
    "\n",
    "user_in = Input(shape=(1,), dtype='int64', name='user_in')\n",
    "u = Embedding(n_users+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(user_in)\n",
    "\n",
    "questions_in = Input(shape=(1,), dtype='int64', name='questions_in')\n",
    "m = Embedding(n_questions+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(questions_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "x2 = merge([u, m], mode='concat')\n",
    "x2 = Flatten()(x2)\n",
    "x2 = Dropout(0.3)(x2)\n",
    "x2 = Dense(70, activation='relu')(x2)\n",
    "x2 = Dropout(0.75)(x2)\n",
    "x2 = Dense(1)(x2)\n",
    "x2 = Dropout(0.3)(x2)\n",
    "x2 = Dense(2)(x2)\n",
    "x2 = Activation('softmax')(x2)\n",
    "\n",
    "model_nn = Model([user_in, questions_in], x2)\n",
    "model_nn.compile(SGD(0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:88: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 183089900 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3661797 samples, validate on 451258 samples\n",
      "Epoch 1/8\n",
      "3661797/3661797 [==============================] - 226s - loss: 15.7725 - acc: 0.5178 - val_loss: 15.7723 - val_acc: 0.5142\n",
      "Epoch 2/8\n",
      "3661797/3661797 [==============================] - 242s - loss: 15.7723 - acc: 0.5215 - val_loss: 15.7721 - val_acc: 0.5293\n",
      "Epoch 3/8\n",
      "3661797/3661797 [==============================] - 300s - loss: 15.7720 - acc: 0.5248 - val_loss: 15.7719 - val_acc: 0.5419\n",
      "Epoch 4/8\n",
      "3661797/3661797 [==============================] - 294s - loss: 15.7718 - acc: 0.5277 - val_loss: 15.7716 - val_acc: 0.5505\n",
      "Epoch 5/8\n",
      "3661797/3661797 [==============================] - 282s - loss: 15.7715 - acc: 0.5310 - val_loss: 15.7714 - val_acc: 0.5556\n",
      "Epoch 6/8\n",
      "3661797/3661797 [==============================] - 261s - loss: 15.7714 - acc: 0.5336 - val_loss: 15.7712 - val_acc: 0.5581\n",
      "Epoch 7/8\n",
      "3661797/3661797 [==============================] - 297s - loss: 15.7711 - acc: 0.5363 - val_loss: 15.7709 - val_acc: 0.5591\n",
      "Epoch 8/8\n",
      "3661797/3661797 [==============================] - 315s - loss: 15.7709 - acc: 0.5383 - val_loss: 15.7707 - val_acc: 0.5594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1290c2690>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=8, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nn.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nn.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=2, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.49079546,  0.50920451],\n",
       "       [ 0.49282321,  0.50717676],\n",
       "       [ 0.493507  ,  0.50649297],\n",
       "       ..., \n",
       "       [ 0.49167562,  0.50832438],\n",
       "       [ 0.49246827,  0.5075317 ],\n",
       "       [ 0.49208143,  0.50791854]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_result = model_nn.predict([test.Users_ID,test.Questions], batch_size =200000)\n",
    "softmax_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.around(softmax_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nom = np.sum(np.dot(result,test_category.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "denom = len(test_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_acc = float(nom)/denom\n",
    "final_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2-1) classical neural network - with adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = trn.shape[0]\n",
    "n_questions = 100\n",
    "n_factors = 50\n",
    "\n",
    "user_in = Input(shape=(1,), dtype='int64', name='user_in')\n",
    "u = Embedding(n_users+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(user_in)\n",
    "\n",
    "questions_in = Input(shape=(1,), dtype='int64', name='questions_in')\n",
    "m = Embedding(n_questions+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(questions_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "x2_1 = merge([u, m], mode='concat')\n",
    "x2_1 = Flatten()(x2_1)\n",
    "x2_1 = Dropout(0.3)(x2_1)\n",
    "x2_1 = Dense(70, activation='relu')(x2_1)\n",
    "x2_1 = Dropout(0.75)(x2_1)\n",
    "x2_1 = Dense(1)(x2_1)\n",
    "x2_1 = Dropout(0.3)(x2_1)\n",
    "x2_1 = Dense(2)(x2_1)\n",
    "x2_1 = Activation('softmax')(x2_1)\n",
    "\n",
    "model_nn = Model([user_in, questions_in], x2_1)\n",
    "model_nn.compile(Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:88: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 182996650 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3659932 samples, validate on 451924 samples\n",
      "Epoch 1/4\n",
      "3659932/3659932 [==============================] - 695s - loss: 9.8242 - acc: 0.5580 - val_loss: 4.6808 - val_acc: 0.5670\n",
      "Epoch 2/4\n",
      "3659932/3659932 [==============================] - 648s - loss: 2.7267 - acc: 0.6002 - val_loss: 1.2960 - val_acc: 0.6462\n",
      "Epoch 3/4\n",
      "3659932/3659932 [==============================] - 639s - loss: 0.9336 - acc: 0.6178 - val_loss: 0.6926 - val_acc: 0.6454\n",
      "Epoch 4/4\n",
      "3659932/3659932 [==============================] - 602s - loss: 0.6723 - acc: 0.6199 - val_loss: 0.6367 - val_acc: 0.6489\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12a9c0ad0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=4, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3659932 samples, validate on 451924 samples\n",
      "Epoch 1/4\n",
      "3659932/3659932 [==============================] - 625s - loss: 0.6501 - acc: 0.6245 - val_loss: 0.6304 - val_acc: 0.6562\n",
      "Epoch 2/4\n",
      "3659932/3659932 [==============================] - 586s - loss: 0.6457 - acc: 0.6312 - val_loss: 0.6274 - val_acc: 0.6620\n",
      "Epoch 3/4\n",
      "3659932/3659932 [==============================] - 533s - loss: 0.6436 - acc: 0.6341 - val_loss: 0.6252 - val_acc: 0.6650\n",
      "Epoch 4/4\n",
      "3659932/3659932 [==============================] - 573s - loss: 0.6420 - acc: 0.6355 - val_loss: 0.6231 - val_acc: 0.6655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12fe1ae90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_nn.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=4, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nn.optimizer.lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nn.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=2, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.48165077,  0.51834929],\n",
       "       [ 0.5470255 ,  0.45297453],\n",
       "       [ 0.53223163,  0.4677684 ],\n",
       "       ..., \n",
       "       [ 0.7566371 ,  0.24336289],\n",
       "       [ 0.74711603,  0.25288397],\n",
       "       [ 0.77893507,  0.22106494]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_result = model_nn.predict([test.Users_ID,test.Questions], batch_size =200000)\n",
    "softmax_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.around(softmax_result)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407344"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407344"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "score = 0\n",
    "\n",
    "for i in range(len(result)):\n",
    "    if result[i][0] == test_category[i][0]:\n",
    "        score+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271289"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6659948348324758"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(score) / len(test_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3)  neural network 2 [dropout -> batchnormalization || + expanded weights] : Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_input(name, n_in, n_out, reg):\n",
    "    inp = Input(shape=(1,), dtype='int64', name=name)\n",
    "    return inp, Embedding(n_in, n_out, input_length=1, embeddings_regularizer=regularizers.l2(reg))(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_in, u = embedding_input('user_in', n_users, n_factors, 1e-4)\n",
    "questions_in, m = embedding_input('movie_in', n_questions, n_factors, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bias(inp, n_in):\n",
    "    x = Embedding(n_in, 1, input_length=1)(inp)\n",
    "    return Flatten()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ub = create_bias(user_in, n_users)\n",
    "qb = create_bias(questions_in, n_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = trn.shape[0]\n",
    "n_questions = 100\n",
    "n_factors = 50\n",
    "\n",
    "user_in = Input(shape=(1,), dtype='int64', name='user_in')\n",
    "u = Embedding(n_users+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(user_in)\n",
    "\n",
    "questions_in = Input(shape=(1,), dtype='int64', name='questions_in')\n",
    "m = Embedding(n_questions+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(questions_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if __name__ == '__main__':\n",
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n"
     ]
    }
   ],
   "source": [
    "x2 = merge([u, m], mode='concat')\n",
    "x2 = Flatten()(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Dense(1000, activation='relu')(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Dense(200, activation='relu')(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Dense(200, activation='relu')(x2)\n",
    "x2 = BatchNormalization()(x2)\n",
    "x2 = Dense(2)(x2)\n",
    "x2 = Activation('softmax')(x2)\n",
    "\n",
    "model_nn2 = Model([user_in, questions_in], x2)\n",
    "model_nn2.compile(Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py:88: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 132014850 elements. This may consume a large amount of memory.\n",
      "  \"This may consume a large amount of memory.\" % num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2640296 samples, validate on 903561 samples\n",
      "Epoch 1/8\n",
      "2640296/2640296 [==============================] - 1310s - loss: 8.4550 - acc: 0.5923 - val_loss: 5.0750 - val_acc: 0.5588\n",
      "Epoch 2/8\n",
      "2640296/2640296 [==============================] - 1424s - loss: 3.4796 - acc: 0.6554 - val_loss: 2.1155 - val_acc: 0.5588\n",
      "Epoch 3/8\n",
      "2640296/2640296 [==============================] - 1399s - loss: 1.4721 - acc: 0.6861 - val_loss: 1.1066 - val_acc: 0.5588\n",
      "Epoch 4/8\n",
      "1200000/2640296 [============>.................] - ETA: 687s - loss: 0.9054 - acc: 0.7137"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-af06dd34b647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_nn2.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=8, \n\u001b[0;32m----> 2\u001b[0;31m           validation_data=([val.Users_ID, val.Questions], val_category))\n\u001b[0m",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1423\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_nn2.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=8, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nn.optimizer.lr=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nn2.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=3, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nn.optimizer.lr=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_nn2.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=8, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4)  neural network 3 [dropout -> batch normalization] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = trn.shape[0]\n",
    "n_questions = 100\n",
    "n_factors = 50\n",
    "\n",
    "user_in = Input(shape=(1,), dtype='int64', name='user_in')\n",
    "u = Embedding(n_users+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(user_in)\n",
    "\n",
    "questions_in = Input(shape=(1,), dtype='int64', name='questions_in')\n",
    "m = Embedding(n_questions+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(questions_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "x4 = merge([u, m], mode='concat')\n",
    "x4 = Flatten()(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Dense(70, activation='relu')(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Dense(30, activation='relu')(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Dense(10, activation='relu')(x4)\n",
    "x4 = BatchNormalization()(x4)\n",
    "x4 = Dense(2)(x4)\n",
    "x4 = Activation('softmax')(x4)\n",
    "\n",
    "model_nn3 = Model([user_in, questions_in], x4)\n",
    "model_nn3.compile(Adam(0.001), loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2640296 samples, validate on 903561 samples\n",
      "Epoch 1/8\n",
      "2640296/2640296 [==============================] - 301s - loss: 8.5076 - acc: 0.5673 - val_loss: 5.0792 - val_acc: 0.5591\n",
      "Epoch 2/8\n",
      "2640296/2640296 [==============================] - 284s - loss: 3.4908 - acc: 0.6493 - val_loss: 2.1152 - val_acc: 0.5588\n",
      "Epoch 3/8\n",
      "2640296/2640296 [==============================] - 257s - loss: 1.4850 - acc: 0.6780 - val_loss: 1.1054 - val_acc: 0.5588\n",
      "Epoch 4/8\n",
      " 200000/2640296 [=>............................] - ETA: 213s - loss: 1.0046 - acc: 0.6946"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ec2b4269ddf4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_nn3.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=8, \n\u001b[0;32m----> 2\u001b[0;31m           validation_data=([val.Users_ID, val.Questions], val_category))\n\u001b[0m",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1423\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_nn3.fit([trn.Users_ID, trn.Questions], train_category, batch_size=200000, nb_epoch=8, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (4)  neural network 4 [Matrix Factorization] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4519200"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort the dataframe\n",
    "ratings.sort(columns=['Users_ID'], inplace=True)\n",
    "# set the index to be this and don't drop\n",
    "ratings.set_index(keys=['Users_ID'], drop=False,inplace=True)\n",
    "# get a list of names\n",
    "names=ratings['Users_ID'].unique().tolist()\n",
    "\n",
    "# now we can perform a lookup on a 'view' of the dataframe\n",
    "# now you can query all 'joes'\n",
    "trn = ratings.loc[(ratings.Users_ID>=1) & (ratings.Users_ID<=32000)]\n",
    "val = ratings.loc[(ratings.Users_ID>32000) & (ratings.Users_ID<=44000)]\n",
    "test = ratings.loc[(ratings.Users_ID>44000)]\n",
    "len(test) + len(trn) + len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "trn_cross = pd.crosstab(trn.Users_ID, trn.Questions, trn.Y_or_N , aggfunc=np.sum)\n",
    "trn_cross.to_pickle(model_path + 'trn_cross.pkl')\n",
    "trn_cross = pd.read_pickle(model_path + 'trn_cross.pkl')\n",
    "trn_cross_array = trn_cross.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 1, 1, ..., 1, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 1, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_cross_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.59],\n",
       "       [ 0.45],\n",
       "       [ 0.  ],\n",
       "       ..., \n",
       "       [ 0.43],\n",
       "       [ 0.68],\n",
       "       [ 0.55]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings_mean.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.41, -0.59, -0.59, ..., -0.59,  0.41, -0.59],\n",
       "       [-0.45,  0.55,  0.55, ...,  0.55,  0.55, -0.45],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n",
       "       ..., \n",
       "       [-0.43, -0.43,  0.57, ..., -0.43, -0.43, -0.43],\n",
       "       [ 0.32, -0.68, -0.68, ..., -0.68, -0.68, -0.68],\n",
       "       [-0.55,  0.45, -0.55, ..., -0.55, -0.55, -0.55]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings_mean = np.nanmean(trn_cross_array, axis=1)\n",
    "trn_cross_array_demeaned = trn_cross_array - user_ratings_mean.reshape(-1,1)\n",
    "trn_cross_array_demeaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, sigma, Vt =svds(trn_cross_array_demeaned,k=50)\n",
    "sigma = np.diag(sigma)\n",
    "\n",
    "all_user_predicted_ratings = np.dot(np.dot(U,sigma), Vt) + user_ratings_mean.reshape(-1,1)\n",
    "trn_preds_df = pd.DataFrame(all_user_predicted_ratings, columns= trn_cross.columns, index = trn_cross.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = trn.shape[0]\n",
    "n_questions = 100\n",
    "n_factors = 50\n",
    "\n",
    "user_in = Input(shape=(1,), dtype='int64', name='user_in')\n",
    "u = Embedding(n_users+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(user_in)\n",
    "\n",
    "questions_in = Input(shape=(1,), dtype='int64', name='questions_in')\n",
    "m = Embedding(n_questions+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(questions_in)\n",
    "\n",
    "svds_in = Input(shape=(1,), dtype='int64', name='svds_in')\n",
    "s = Embedding(n_questions+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(svds_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:5: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Layer merge_37 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.core.Reshape'>. Full input: [<tf.Tensor 'dense_58/BiasAdd:0' shape=(?, 10) dtype=float32>, <keras.layers.core.Reshape object at 0x15b68a990>]. All inputs to the layer should be tensors.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-af2a8c984238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mx4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mx4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'concat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mx4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/legacy/layers.pyc\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(inputs, mode, concat_axis, dot_axes, output_shape, output_mask, arguments, name)\u001b[0m\n\u001b[1;32m    466\u001b[0m                             \u001b[0marguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m                             name=name)\n\u001b[0;32m--> 468\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;31m# with the input_spec specified in the layer constructor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;31m# Collect input shapes to build layer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/engine/topology.pyc\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    423\u001b[0m                                  \u001b[0;34m'Received type: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full input: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                                  \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. All inputs to the layer '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m                                  'should be tensors.')\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer merge_37 was called with an input that isn't a symbolic tensor. Received type: <class 'keras.layers.core.Reshape'>. Full input: [<tf.Tensor 'dense_58/BiasAdd:0' shape=(?, 10) dtype=float32>, <keras.layers.core.Reshape object at 0x15b68a990>]. All inputs to the layer should be tensors."
     ]
    }
   ],
   "source": [
    "s4 = Flatten()(s)\n",
    "s4 = Dense(10)(s4)\n",
    "s4 = Reshape((10,))\n",
    "\n",
    "x4 = keras.layers.merge([u, m], mode='dot')\n",
    "x4 = Flatten()(x4)\n",
    "x4 = Dense(10)(x4)\n",
    "\n",
    "x4 = keras.layers.merge([x4, s4], mode ='concat')\n",
    "\n",
    "x4 = Dense(2)(x4)\n",
    "x4 = Activation('softmax')(x4)\n",
    "\n",
    "model = Model([user_in, questions_in], x4)\n",
    "model.compile(Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([trn_preds_df.Users_ID, trn_preds_df.Questions], train_category, batch_size=200000, epochs=2, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = trn.shape[0]\n",
    "n_questions = 100\n",
    "n_factors = 50\n",
    "\n",
    "user_in = Input(shape=(1,), dtype='int64', name='user_in')\n",
    "u = Embedding(n_users+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(user_in)\n",
    "\n",
    "questions_in = Input(shape=(1,), dtype='int64', name='questions_in')\n",
    "m = Embedding(n_questions+1, n_factors, input_length=1, embeddings_regularizer=regularizers.l2(1e-4))(questions_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:20: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    }
   ],
   "source": [
    "model_left = Sequential()\n",
    "model_left.add(Embedding(n_users+1, n_factors, input_length=1, embeddings_regularizer= regularizers.l2(1e-4)))\n",
    "model_left.add(Flatten())\n",
    "model_left.add(Dense(64))\n",
    "model_left.add(Activation('relu'))\n",
    "model_left.add(Dense(82))\n",
    "model_left.add(Activation('relu'))\n",
    "model_left.add(Dense(10))\n",
    "\n",
    "model_right = Sequential()\n",
    "model_right.add(Embedding(n_questions+1, n_factors, input_length=1, embeddings_regularizer= regularizers.l2(1e-4)))\n",
    "model_right.add(Flatten())\n",
    "model_right.add(Dense(24))\n",
    "model_right.add(Activation('relu'))\n",
    "model_right.add(Dense(30))\n",
    "model_right.add(Activation('relu'))\n",
    "model_right.add(Dense(10))\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Merge([model_left, model_right], mode='concat'))\n",
    "model2.add(Dense(10))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(20))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dense(10))\n",
    "model2.add(Dense(2)) \n",
    "model2.add(Activation('softmax'))\n",
    "\n",
    "model2.compile(Adam(0.001), loss='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/models.py:851: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3659932 samples, validate on 451924 samples\n",
      "Epoch 1/6\n",
      "3659932/3659932 [==============================] - 461s - loss: 10.8248 - acc: 0.5547 - val_loss: 6.1662 - val_acc: 0.5583\n",
      "Epoch 2/6\n",
      " 750000/3659932 [=====>........................] - ETA: 323s - loss: 5.7650 - acc: 0.5578"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9a54cdf914a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model2.fit([trn.Users_ID, trn.Questions], train_category, batch_size=250000, nb_epoch=6, \n\u001b[0;32m----> 2\u001b[0;31m           validation_data=([val.Users_ID, val.Questions], val_category))\n\u001b[0m",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1423\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1424\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1425\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1075\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/choelinbeom/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model2.fit([trn.Users_ID, trn.Questions], train_category, batch_size=250000, nb_epoch=6, \n",
    "          validation_data=([val.Users_ID, val.Questions], val_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'weight_mind_riiid.h5')\n",
    "model.load_weights(model_path+'weight_mind_riiid.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975552/975552 [==============================] - 5s     \n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate([test.Users_ID, test.Questions], test.Y_or_N, batch_size =250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.214141383292027"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "merge_2 (Merge)              (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 20)                220       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 131,975,359\n",
      "Trainable params: 131,975,359\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### colaborative filtering - binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_users= ratings['Users_ID'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.pivot_table(ratings,index=[\"Users_ID\"],\n",
    "               columns=[\"Questions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighbor_Based_classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross.astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3616013"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = np.random.rand(len(ratings)) < 0.8\n",
    "trn = ratings[msk]\n",
    "val = ratings[~msk]\n",
    "\n",
    "n_users = trn.shape[0]\n",
    "n_questions = 100\n",
    "\n",
    "n_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_not = ~cross.astype(bool)\n",
    "cross_not = cross_not.astype(int)\n",
    "cross_not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "not_unit1 = user_by_user_not.loc[:4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "distance = unit1_not + u1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance.applymap(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x == 45192: return 0\n",
    "    else :return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([    9,    13,    14,    15,    16,    17,    20,    21,    22,\n",
       "               26,\n",
       "            ...\n",
       "            54095, 54096, 54097, 54098, 54100, 54101, 54102, 54103, 54104,\n",
       "            54106],\n",
       "           dtype='int64', name=u'Users_ID', length=45192)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conc_b1 = pd.concat([conc1, conc2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conc_b2 = pd.concat([conc3, conc4, conc5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "concate_complete = pd.concat([conc_b1, conc_b2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    \n",
    "    unit_number = i+13\n",
    "    \n",
    "    temp = pd.read_pickle(model_path + 'user_by_user_u'+ str(unit_number) + '.pkl')\n",
    "    \n",
    "    user_by_user = pd.concat([user_by_user, temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "True_by_True_users = np.dot(csr_matrix(cross_array),csr_matrix(cross_array.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Save_Users_array = model_path + 'TruebyTrue.txt'\n",
    "np.savetxt(Save_Users_array, True_by_True_users, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "True_by_True_questions = np.dot(cross_array.T,cross_array)\n",
    "Save_Question_Neighbors_array = model_path + 'TruebyTrue.txt'\n",
    "np.savetxt(Save_Question_Neighbors_array, True_by_True_questions, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "False_by_False = np.dot(~cross_array,~cross_array.T)\n",
    "also_False_by_False = True_by_True + False_by_False\n",
    "SaveAt2 = model_path + 'TruebyTrue_Plus_FalsebyFalse.txt'\n",
    "np.savetxt(SaveAt2, also_False_by_False, fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True, False],\n",
       "       [False,  True, False],\n",
       "       [ True,  True,  True],\n",
       "       [ True, False, False]], dtype=bool)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def xor_dot(x,y):\n",
    "    for i in range(len(x)):\n",
    "        out = out.append(np.logical_xor(x[i+1],(y[:,i+1])))\n",
    "\n",
    "    \n",
    "kk = [[True, True, False],[False, True, False], [True, True, True],[True, False, False]]\n",
    "kkk = [[False, False, False,False],[False, True, False,False], [False,False ,True, True]]\n",
    "\n",
    "\n",
    "\n",
    "kk = np.array(kk)\n",
    "kkk = np.array(kkk)\n",
    "\n",
    "~kk\n",
    "kk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights(model_path+'bias.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_path+'bias.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "movie_pca = pca.fit(movie_emb.T).components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fac0 = movie_pca[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_comp = [(f, movie_names[movies[i]]) for f,i in zip(fac0, topMovies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(movie_comp, key=itemgetter(0), reverse=True)[:10]\n",
    "sorted(movie_comp, key=itemgetter(0))[:10]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
